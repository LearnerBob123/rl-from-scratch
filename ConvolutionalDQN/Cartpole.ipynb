{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acac74e",
   "metadata": {},
   "source": [
    "# Convolutional DQN (C-DQN) for CartPole\n",
    "\n",
    "This notebook demonstrates a simple Convolutional DQN that uses a stack of 4 grayscale frames (84Ã—84) as input. It's intended as an educational example; not all hyperparameters are tuned for performance.\n",
    "\n",
    "Instructions:\n",
    "- Run the cells in order.\n",
    "- Ensure dependencies are installed (see the cell below).\n",
    "- The notebook includes small checks to validate tensor shapes and a minimal evaluation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1258bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcb7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Frame Preprocessing Functions ---\n",
    "\n",
    "def resizer(frame):\n",
    "    \"\"\"\n",
    "    Converts RGB frame to grayscale and resizes it to 84x84.\n",
    "    The formula is a standard luminance calculation.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale using the standard weighted sum\n",
    "    # frame[:, :, 0] is R, [:, :, 1] is G, [:, :, 2] is B\n",
    "    gray = 0.299 * frame[:, :, 0] + 0.587 * frame[:, :, 1] + 0.114 * frame[:, :, 2]\n",
    "    \n",
    "    # Resize and normalize\n",
    "    # INTER_AREA is preferred for downsampling\n",
    "    resized_frame = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA).astype(np.float32) / 255.0\n",
    "    return resized_frame\n",
    "\n",
    "def frame_tensor(resized_frame):\n",
    "    \"\"\"\n",
    "    Convert a single preprocessed frame (84x84 numpy) to a tensor of shape (1, 1, 84, 84).\n",
    "    (Used only for initial single frame processing, not in the main loop).\n",
    "    \"\"\"\n",
    "    arr = np.ascontiguousarray(resized_frame, dtype=np.float32)\n",
    "    return torch.from_numpy(arr).unsqueeze(0).unsqueeze(0) # (1, 1, 84, 84)\n",
    "\n",
    "def stack_to_tensor(frame_stack):\n",
    "    \"\"\"\n",
    "    Convert a sequence (deque) of 4 frames (numpy arrays) to a tensor\n",
    "    of shape (1, 4, 84, 84) suitable for the ConvNet.\n",
    "    This function expects frames in the deque to be 84x84 np.float32 arrays.\n",
    "    \"\"\"\n",
    "    # np.stack creates an array of shape (4, 84, 84)\n",
    "    state = np.stack(list(frame_stack), axis=0).astype(np.float32)\n",
    "    # torch.from_numpy and unsqueeze(0) converts to (1, 4, 84, 84)\n",
    "    return torch.from_numpy(state).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64052ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DQN Network ---\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    # \n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input: (N, 4, 84, 84) where 4 is the number of stacked frames\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=8, stride=4) # Output: ((84-8)/4 + 1) = 20x20\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2) # Output: ((20-4)/2 + 1) = 9x9\n",
    "\n",
    "        conv_out_size = self._get_conv_out() # Calculated as 32 * 9 * 9 = 2592\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
    "        self.fc2 = nn.Linear(256, action_space)\n",
    "\n",
    "    def _get_conv_out(self):\n",
    "        # Calculates the size of the output feature map after convolutions and flattening\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 4, 84, 84)\n",
    "            x = F.relu(self.conv1(dummy))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            # x.view(1, -1) flattens the dimensions after the batch dimension (1)\n",
    "            # .size(1) returns the number of features\n",
    "            return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        q = self.fc2(x)\n",
    "        return q\n",
    "\n",
    "# --- Action Selection ---\n",
    "\n",
    "def epsilon_greedy_action_selection(q_values, epsilon):\n",
    "    # q_values: shape (1, num_actions) or (batch_size, num_actions)\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        # Explore: pick a random action\n",
    "        return random.randrange(q_values.shape[1])\n",
    "    else:\n",
    "        # Exploit: pick the action with the highest Q-value\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # NOTE: To save memory and simplify batching, the buffer stores \n",
    "    # the (4, 84, 84) NumPy array state stack, not the (1, 4, 84, 84) tensor.\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        state, next_state: NumPy arrays (4, 84, 84)\n",
    "        \"\"\"\n",
    "        # Store as NumPy arrays/standard types\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert states back to a batched tensor of shape (batch_size, 4, 84, 84)\n",
    "        states = torch.from_numpy(np.array(states)).float()\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Training Function ---\n",
    "\n",
    "def train_step(buffer, net, target_net, optimizer, batch_size, gamma):\n",
    "    # \n",
    "    if len(buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "    # Convert sampled lists/tuples to tensors\n",
    "    actions = torch.tensor(actions, dtype=torch.long) # (batch_size,)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32) # (batch_size,)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32) # (batch_size,)\n",
    "\n",
    "    # Compute Q-values for current state (Q(s, a))\n",
    "    # net(states) has shape (batch_size, num_actions)\n",
    "    # gather(1, actions.unsqueeze(1)) selects the Q-value for the action that was taken\n",
    "    q_values = net(states).gather(1, actions.unsqueeze(1)).squeeze(1) # (batch_size,)\n",
    "\n",
    "    # Compute target Q-values (r + gamma * max_a' Q_target(s', a'))\n",
    "    with torch.no_grad():\n",
    "        # Get Q-values for next state from the TARGET network\n",
    "        next_q_values = target_net(next_states) # (batch_size, num_actions)\n",
    "        # Find the maximum Q-value for the next state\n",
    "        max_next_q_values = next_q_values.max(1)[0] # (batch_size,)\n",
    "        # Target Q-value: r + gamma * max_Q_target(s')\n",
    "        # (1 - dones) handles terminal states: if done is True (1), the max_next_q_values term is zeroed out.\n",
    "        target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "    # Compute loss (MSE between Q(s,a) and Target Q-value)\n",
    "    loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient clipping is often used in DQN, but omitted here for simplicity\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN Training (CNN on Frames)\n",
      "Episode    1 | Reward:  15.00 | Steps: 15 | Avg Loss: 0.0000 | Epsilon: 0.895 | Buffer:    15 | Max Q: 0.022\n",
      "Episode    2 | Reward:  19.00 | Steps: 34 | Avg Loss: 0.0000 | Epsilon: 0.891 | Buffer:    34 | Max Q: 0.023\n",
      "Episode    3 | Reward:  18.00 | Steps: 52 | Avg Loss: 0.0000 | Epsilon: 0.887 | Buffer:    52 | Max Q: 0.023\n",
      "Episode    4 | Reward:  24.00 | Steps: 76 | Avg Loss: 0.4059 | Epsilon: 0.882 | Buffer:    76 | Max Q: 1.463\n",
      "Episode    5 | Reward:  30.00 | Steps: 106 | Avg Loss: 0.0146 | Epsilon: 0.878 | Buffer:   106 | Max Q: 0.998\n",
      "Episode    6 | Reward:  14.00 | Steps: 120 | Avg Loss: 0.0007 | Epsilon: 0.873 | Buffer:   120 | Max Q: 1.048\n",
      "Episode    7 | Reward:  16.00 | Steps: 136 | Avg Loss: 0.0002 | Epsilon: 0.869 | Buffer:   136 | Max Q: 1.019\n",
      "Episode    8 | Reward:  23.00 | Steps: 159 | Avg Loss: 0.0001 | Epsilon: 0.865 | Buffer:   159 | Max Q: 1.027\n",
      "Episode    9 | Reward:  10.00 | Steps: 169 | Avg Loss: 0.0000 | Epsilon: 0.860 | Buffer:   169 | Max Q: 1.024\n",
      "Episode   10 | Reward:  33.00 | Steps: 202 | Avg Loss: 0.0000 | Epsilon: 0.856 | Buffer:   202 | Max Q: 1.026\n",
      "Episode   11 | Reward:  15.00 | Steps: 217 | Avg Loss: 0.0000 | Epsilon: 0.852 | Buffer:   217 | Max Q: 1.024\n",
      "Episode   12 | Reward:  26.00 | Steps: 243 | Avg Loss: 0.0000 | Epsilon: 0.847 | Buffer:   243 | Max Q: 1.023\n",
      "\n",
      "--- Episode 13: Updated target network. ---\n",
      "Episode   13 | Reward:  37.00 | Steps: 280 | Avg Loss: 0.1370 | Epsilon: 0.843 | Buffer:   280 | Max Q: 2.066\n",
      "Episode   14 | Reward:  13.00 | Steps: 293 | Avg Loss: 0.0617 | Epsilon: 0.839 | Buffer:   293 | Max Q: 1.962\n",
      "Episode   15 | Reward:  13.00 | Steps: 306 | Avg Loss: 0.0489 | Epsilon: 0.835 | Buffer:   306 | Max Q: 1.964\n",
      "Episode   16 | Reward:  18.00 | Steps: 324 | Avg Loss: 0.0394 | Epsilon: 0.831 | Buffer:   324 | Max Q: 2.018\n",
      "Episode   17 | Reward:  17.00 | Steps: 341 | Avg Loss: 0.0516 | Epsilon: 0.826 | Buffer:   341 | Max Q: 1.966\n",
      "Episode   18 | Reward:  26.00 | Steps: 367 | Avg Loss: 0.0533 | Epsilon: 0.822 | Buffer:   367 | Max Q: 2.000\n",
      "Episode   19 | Reward:  36.00 | Steps: 403 | Avg Loss: 0.0489 | Epsilon: 0.818 | Buffer:   403 | Max Q: 1.917\n",
      "Episode   20 | Reward:  46.00 | Steps: 449 | Avg Loss: 0.0454 | Epsilon: 0.814 | Buffer:   449 | Max Q: 2.028\n",
      "Episode   21 | Reward:  13.00 | Steps: 462 | Avg Loss: 0.0442 | Epsilon: 0.810 | Buffer:   462 | Max Q: 2.048\n",
      "Episode   22 | Reward:  11.00 | Steps: 473 | Avg Loss: 0.0421 | Epsilon: 0.806 | Buffer:   473 | Max Q: 1.967\n",
      "Episode   23 | Reward:  14.00 | Steps: 487 | Avg Loss: 0.0357 | Epsilon: 0.802 | Buffer:   487 | Max Q: 1.977\n",
      "\n",
      "--- Episode 24: Updated target network. ---\n",
      "Episode   24 | Reward:  18.00 | Steps: 505 | Avg Loss: 0.2371 | Epsilon: 0.798 | Buffer:   505 | Max Q: 2.374\n",
      "Episode   25 | Reward:  66.00 | Steps: 571 | Avg Loss: 0.2024 | Epsilon: 0.794 | Buffer:   571 | Max Q: 2.609\n",
      "Episode   26 | Reward:  10.00 | Steps: 581 | Avg Loss: 0.1874 | Epsilon: 0.790 | Buffer:   581 | Max Q: 3.160\n",
      "Episode   27 | Reward:  16.00 | Steps: 597 | Avg Loss: 0.2044 | Epsilon: 0.786 | Buffer:   597 | Max Q: 2.783\n",
      "Episode   28 | Reward:  20.00 | Steps: 617 | Avg Loss: 0.1835 | Epsilon: 0.782 | Buffer:   617 | Max Q: 2.734\n",
      "Episode   29 | Reward:  39.00 | Steps: 656 | Avg Loss: 0.1567 | Epsilon: 0.778 | Buffer:   656 | Max Q: 2.884\n",
      "Episode   30 | Reward:  13.00 | Steps: 669 | Avg Loss: 0.1606 | Epsilon: 0.774 | Buffer:   669 | Max Q: 3.008\n",
      "Episode   31 | Reward:  19.00 | Steps: 688 | Avg Loss: 0.1950 | Epsilon: 0.770 | Buffer:   688 | Max Q: 2.948\n",
      "Episode   32 | Reward:  12.00 | Steps: 700 | Avg Loss: 0.1726 | Epsilon: 0.767 | Buffer:   700 | Max Q: 2.978\n",
      "Episode   33 | Reward:  24.00 | Steps: 724 | Avg Loss: 0.1615 | Epsilon: 0.763 | Buffer:   724 | Max Q: 2.980\n",
      "Episode   34 | Reward:  18.00 | Steps: 742 | Avg Loss: 0.1646 | Epsilon: 0.759 | Buffer:   742 | Max Q: 2.772\n",
      "\n",
      "--- Episode 35: Updated target network. ---\n",
      "Episode   35 | Reward:  16.00 | Steps: 758 | Avg Loss: 0.5877 | Epsilon: 0.755 | Buffer:   758 | Max Q: 4.839\n",
      "Episode   36 | Reward:  10.00 | Steps: 768 | Avg Loss: 0.5925 | Epsilon: 0.751 | Buffer:   768 | Max Q: 3.711\n",
      "Episode   37 | Reward:  19.00 | Steps: 787 | Avg Loss: 0.3768 | Epsilon: 0.748 | Buffer:   787 | Max Q: 3.764\n",
      "Episode   38 | Reward:  28.00 | Steps: 815 | Avg Loss: 0.4028 | Epsilon: 0.744 | Buffer:   815 | Max Q: 4.163\n",
      "Episode   39 | Reward:  13.00 | Steps: 828 | Avg Loss: 0.3483 | Epsilon: 0.740 | Buffer:   828 | Max Q: 4.149\n",
      "Episode   40 | Reward:  20.00 | Steps: 848 | Avg Loss: 0.4162 | Epsilon: 0.736 | Buffer:   848 | Max Q: 3.773\n",
      "Episode   41 | Reward:  11.00 | Steps: 859 | Avg Loss: 0.3676 | Epsilon: 0.733 | Buffer:   859 | Max Q: 3.820\n",
      "Episode   42 | Reward:  12.00 | Steps: 871 | Avg Loss: 0.4078 | Epsilon: 0.729 | Buffer:   871 | Max Q: 3.423\n",
      "Episode   43 | Reward:  16.00 | Steps: 887 | Avg Loss: 0.3955 | Epsilon: 0.725 | Buffer:   887 | Max Q: 3.989\n",
      "Episode   44 | Reward:  14.00 | Steps: 901 | Avg Loss: 0.3811 | Epsilon: 0.722 | Buffer:   901 | Max Q: 3.594\n",
      "Episode   45 | Reward:  13.00 | Steps: 914 | Avg Loss: 0.3568 | Epsilon: 0.718 | Buffer:   914 | Max Q: 3.788\n",
      "Episode   46 | Reward:  49.00 | Steps: 963 | Avg Loss: 0.3777 | Epsilon: 0.715 | Buffer:   963 | Max Q: 3.523\n",
      "Episode   47 | Reward:  18.00 | Steps: 981 | Avg Loss: 0.4186 | Epsilon: 0.711 | Buffer:   981 | Max Q: 2.906\n",
      "Episode   48 | Reward:  15.00 | Steps: 996 | Avg Loss: 0.4895 | Epsilon: 0.708 | Buffer:   996 | Max Q: 2.994\n",
      "\n",
      "--- Episode 49: Updated target network. ---\n",
      "Episode   49 | Reward:  51.00 | Steps: 1047 | Avg Loss: 0.5693 | Epsilon: 0.704 | Buffer:  1047 | Max Q: 4.750\n",
      "Episode   50 | Reward:  11.00 | Steps: 1058 | Avg Loss: 0.5098 | Epsilon: 0.700 | Buffer:  1058 | Max Q: 4.253\n",
      "Episode   51 | Reward:  11.00 | Steps: 1069 | Avg Loss: 0.4015 | Epsilon: 0.697 | Buffer:  1069 | Max Q: 3.512\n",
      "Episode   52 | Reward:  21.00 | Steps: 1090 | Avg Loss: 0.4207 | Epsilon: 0.693 | Buffer:  1090 | Max Q: 3.916\n",
      "Episode   53 | Reward:  18.00 | Steps: 1108 | Avg Loss: 0.5190 | Epsilon: 0.690 | Buffer:  1108 | Max Q: 3.852\n",
      "Episode   54 | Reward:  18.00 | Steps: 1126 | Avg Loss: 0.4691 | Epsilon: 0.687 | Buffer:  1126 | Max Q: 3.851\n",
      "Episode   55 | Reward:  13.00 | Steps: 1139 | Avg Loss: 0.4538 | Epsilon: 0.683 | Buffer:  1139 | Max Q: 4.187\n",
      "Episode   56 | Reward:  10.00 | Steps: 1149 | Avg Loss: 0.6403 | Epsilon: 0.680 | Buffer:  1149 | Max Q: 3.401\n",
      "Episode   57 | Reward:  13.00 | Steps: 1162 | Avg Loss: 0.5468 | Epsilon: 0.676 | Buffer:  1162 | Max Q: 3.131\n",
      "Episode   58 | Reward:  20.00 | Steps: 1182 | Avg Loss: 0.4628 | Epsilon: 0.673 | Buffer:  1182 | Max Q: 3.295\n",
      "Episode   59 | Reward:  39.00 | Steps: 1221 | Avg Loss: 0.5464 | Epsilon: 0.670 | Buffer:  1221 | Max Q: 5.252\n",
      "\n",
      "--- Episode 60: Updated target network. ---\n",
      "Episode   60 | Reward:  39.00 | Steps: 1260 | Avg Loss: 0.5871 | Epsilon: 0.666 | Buffer:  1260 | Max Q: 6.073\n",
      "Episode   61 | Reward:  33.00 | Steps: 1293 | Avg Loss: 0.7743 | Epsilon: 0.663 | Buffer:  1293 | Max Q: 3.625\n",
      "Episode   62 | Reward:  16.00 | Steps: 1309 | Avg Loss: 0.8364 | Epsilon: 0.660 | Buffer:  1309 | Max Q: 3.701\n",
      "Episode   63 | Reward:  24.00 | Steps: 1333 | Avg Loss: 0.7651 | Epsilon: 0.656 | Buffer:  1333 | Max Q: 4.100\n",
      "Episode   64 | Reward:  31.00 | Steps: 1364 | Avg Loss: 0.7118 | Epsilon: 0.653 | Buffer:  1364 | Max Q: 3.612\n",
      "Episode   65 | Reward:  48.00 | Steps: 1412 | Avg Loss: 0.6623 | Epsilon: 0.650 | Buffer:  1412 | Max Q: 4.522\n",
      "Episode   66 | Reward:  17.00 | Steps: 1429 | Avg Loss: 0.6757 | Epsilon: 0.646 | Buffer:  1429 | Max Q: 3.826\n",
      "Episode   67 | Reward:  27.00 | Steps: 1456 | Avg Loss: 0.5990 | Epsilon: 0.643 | Buffer:  1456 | Max Q: 3.531\n",
      "Episode   68 | Reward:  19.00 | Steps: 1475 | Avg Loss: 0.6926 | Epsilon: 0.640 | Buffer:  1475 | Max Q: 6.193\n",
      "Episode   69 | Reward:  18.00 | Steps: 1493 | Avg Loss: 0.4678 | Epsilon: 0.637 | Buffer:  1493 | Max Q: 4.058\n",
      "\n",
      "--- Episode 70: Updated target network. ---\n",
      "Episode   70 | Reward:  16.00 | Steps: 1509 | Avg Loss: 1.0046 | Epsilon: 0.634 | Buffer:  1509 | Max Q: 3.998\n",
      "Episode   71 | Reward:  13.00 | Steps: 1522 | Avg Loss: 0.6613 | Epsilon: 0.630 | Buffer:  1522 | Max Q: 7.189\n",
      "Episode   72 | Reward:   9.00 | Steps: 1531 | Avg Loss: 0.7778 | Epsilon: 0.627 | Buffer:  1531 | Max Q: 6.624\n",
      "Episode   73 | Reward:  17.00 | Steps: 1548 | Avg Loss: 1.0565 | Epsilon: 0.624 | Buffer:  1548 | Max Q: 3.733\n",
      "Episode   74 | Reward:  18.00 | Steps: 1566 | Avg Loss: 0.8082 | Epsilon: 0.621 | Buffer:  1566 | Max Q: 4.039\n",
      "Episode   75 | Reward:  31.00 | Steps: 1597 | Avg Loss: 1.1233 | Epsilon: 0.618 | Buffer:  1597 | Max Q: 3.909\n",
      "Episode   76 | Reward:  33.00 | Steps: 1630 | Avg Loss: 0.9002 | Epsilon: 0.615 | Buffer:  1630 | Max Q: 7.104\n",
      "Episode   77 | Reward:  11.00 | Steps: 1641 | Avg Loss: 0.7963 | Epsilon: 0.612 | Buffer:  1641 | Max Q: 4.643\n",
      "Episode   78 | Reward:   9.00 | Steps: 1650 | Avg Loss: 1.0213 | Epsilon: 0.609 | Buffer:  1650 | Max Q: 6.659\n",
      "Episode   79 | Reward:  27.00 | Steps: 1677 | Avg Loss: 0.9042 | Epsilon: 0.606 | Buffer:  1677 | Max Q: 6.476\n",
      "Episode   80 | Reward:  20.00 | Steps: 1697 | Avg Loss: 0.9290 | Epsilon: 0.603 | Buffer:  1697 | Max Q: 6.954\n",
      "Episode   81 | Reward:  26.00 | Steps: 1723 | Avg Loss: 0.8058 | Epsilon: 0.600 | Buffer:  1723 | Max Q: 3.670\n",
      "Episode   82 | Reward:  23.00 | Steps: 1746 | Avg Loss: 0.7956 | Epsilon: 0.597 | Buffer:  1746 | Max Q: 3.466\n",
      "\n",
      "--- Episode 83: Updated target network. ---\n",
      "Episode   83 | Reward:   9.00 | Steps: 1755 | Avg Loss: 1.0114 | Epsilon: 0.594 | Buffer:  1755 | Max Q: 5.871\n",
      "Episode   84 | Reward:  27.00 | Steps: 1782 | Avg Loss: 1.2248 | Epsilon: 0.591 | Buffer:  1782 | Max Q: 4.469\n",
      "Episode   85 | Reward:  19.00 | Steps: 1801 | Avg Loss: 0.9349 | Epsilon: 0.588 | Buffer:  1801 | Max Q: 3.610\n",
      "Episode   86 | Reward:  16.00 | Steps: 1817 | Avg Loss: 1.2677 | Epsilon: 0.585 | Buffer:  1817 | Max Q: 3.175\n",
      "Episode   87 | Reward:   9.00 | Steps: 1826 | Avg Loss: 0.9856 | Epsilon: 0.582 | Buffer:  1826 | Max Q: 5.912\n",
      "Episode   88 | Reward:  30.00 | Steps: 1856 | Avg Loss: 1.0923 | Epsilon: 0.579 | Buffer:  1856 | Max Q: 7.064\n",
      "Episode   89 | Reward:  16.00 | Steps: 1872 | Avg Loss: 1.3837 | Epsilon: 0.576 | Buffer:  1872 | Max Q: 7.769\n",
      "Episode   90 | Reward:  32.00 | Steps: 1904 | Avg Loss: 0.9614 | Epsilon: 0.573 | Buffer:  1904 | Max Q: 3.158\n",
      "Episode   91 | Reward:  14.00 | Steps: 1918 | Avg Loss: 1.0244 | Epsilon: 0.570 | Buffer:  1918 | Max Q: 5.734\n",
      "Episode   92 | Reward:   9.00 | Steps: 1927 | Avg Loss: 1.2566 | Epsilon: 0.568 | Buffer:  1927 | Max Q: 4.253\n",
      "Episode   93 | Reward:  13.00 | Steps: 1940 | Avg Loss: 0.9893 | Epsilon: 0.565 | Buffer:  1940 | Max Q: 3.582\n",
      "Episode   94 | Reward:  13.00 | Steps: 1953 | Avg Loss: 0.7274 | Epsilon: 0.562 | Buffer:  1953 | Max Q: 4.502\n",
      "Episode   95 | Reward:  26.00 | Steps: 1979 | Avg Loss: 1.0601 | Epsilon: 0.559 | Buffer:  1979 | Max Q: 8.328\n",
      "\n",
      "--- Episode 96: Updated target network. ---\n",
      "Episode   96 | Reward:  44.00 | Steps: 2023 | Avg Loss: 1.1854 | Epsilon: 0.556 | Buffer:  2023 | Max Q: 5.221\n",
      "Episode   97 | Reward:  16.00 | Steps: 2039 | Avg Loss: 0.7680 | Epsilon: 0.553 | Buffer:  2039 | Max Q: 3.235\n",
      "Episode   98 | Reward:  14.00 | Steps: 2053 | Avg Loss: 1.0865 | Epsilon: 0.551 | Buffer:  2053 | Max Q: 3.118\n",
      "Episode   99 | Reward:  45.00 | Steps: 2098 | Avg Loss: 1.3758 | Epsilon: 0.548 | Buffer:  2098 | Max Q: 2.946\n",
      "Saved checkpoint at episode 100\n",
      "Episode  100 | Reward:  11.00 | Steps: 2109 | Avg Loss: 1.2243 | Epsilon: 0.545 | Buffer:  2109 | Max Q: 8.224\n",
      "Episode  101 | Reward:  37.00 | Steps: 2146 | Avg Loss: 1.0935 | Epsilon: 0.542 | Buffer:  2146 | Max Q: 2.847\n",
      "Episode  102 | Reward:  15.00 | Steps: 2161 | Avg Loss: 1.0022 | Epsilon: 0.540 | Buffer:  2161 | Max Q: 3.929\n",
      "Episode  103 | Reward:  25.00 | Steps: 2186 | Avg Loss: 0.9296 | Epsilon: 0.537 | Buffer:  2186 | Max Q: 3.841\n",
      "Episode  104 | Reward:  15.00 | Steps: 2201 | Avg Loss: 1.0519 | Epsilon: 0.534 | Buffer:  2201 | Max Q: 3.745\n",
      "Episode  105 | Reward:  10.00 | Steps: 2211 | Avg Loss: 1.5372 | Epsilon: 0.532 | Buffer:  2211 | Max Q: 4.668\n",
      "Episode  106 | Reward:  35.00 | Steps: 2246 | Avg Loss: 1.0023 | Epsilon: 0.529 | Buffer:  2246 | Max Q: 3.604\n",
      "\n",
      "--- Episode 107: Updated target network. ---\n",
      "Episode  107 | Reward:  17.00 | Steps: 2263 | Avg Loss: 1.2121 | Epsilon: 0.526 | Buffer:  2263 | Max Q: 8.708\n",
      "Episode  108 | Reward:  15.00 | Steps: 2278 | Avg Loss: 1.3744 | Epsilon: 0.524 | Buffer:  2278 | Max Q: 3.142\n",
      "Episode  109 | Reward:  25.00 | Steps: 2303 | Avg Loss: 1.2913 | Epsilon: 0.521 | Buffer:  2303 | Max Q: 2.517\n",
      "Episode  110 | Reward:  17.00 | Steps: 2320 | Avg Loss: 1.3178 | Epsilon: 0.519 | Buffer:  2320 | Max Q: 5.714\n",
      "Episode  111 | Reward:  12.00 | Steps: 2332 | Avg Loss: 1.1351 | Epsilon: 0.516 | Buffer:  2332 | Max Q: 7.544\n",
      "Episode  112 | Reward:  13.00 | Steps: 2345 | Avg Loss: 1.1437 | Epsilon: 0.513 | Buffer:  2345 | Max Q: 3.357\n",
      "Episode  113 | Reward:  23.00 | Steps: 2368 | Avg Loss: 1.1269 | Epsilon: 0.511 | Buffer:  2368 | Max Q: 9.098\n",
      "Episode  114 | Reward:  12.00 | Steps: 2380 | Avg Loss: 1.2133 | Epsilon: 0.508 | Buffer:  2380 | Max Q: 2.854\n",
      "Episode  115 | Reward:  30.00 | Steps: 2410 | Avg Loss: 1.0570 | Epsilon: 0.506 | Buffer:  2410 | Max Q: 2.567\n",
      "Episode  116 | Reward:  11.00 | Steps: 2421 | Avg Loss: 1.4970 | Epsilon: 0.503 | Buffer:  2421 | Max Q: 5.067\n",
      "Episode  117 | Reward:   9.00 | Steps: 2430 | Avg Loss: 1.6716 | Epsilon: 0.501 | Buffer:  2430 | Max Q: 4.481\n",
      "Episode  118 | Reward:  14.00 | Steps: 2444 | Avg Loss: 1.3354 | Epsilon: 0.498 | Buffer:  2444 | Max Q: 3.110\n",
      "Episode  119 | Reward:  11.00 | Steps: 2455 | Avg Loss: 0.8302 | Epsilon: 0.496 | Buffer:  2455 | Max Q: 3.672\n",
      "Episode  120 | Reward:  37.00 | Steps: 2492 | Avg Loss: 1.0588 | Epsilon: 0.493 | Buffer:  2492 | Max Q: 4.114\n",
      "\n",
      "--- Episode 121: Updated target network. ---\n",
      "Episode  121 | Reward:  13.00 | Steps: 2505 | Avg Loss: 1.3145 | Epsilon: 0.491 | Buffer:  2505 | Max Q: 5.124\n",
      "Episode  122 | Reward:   8.00 | Steps: 2513 | Avg Loss: 1.6371 | Epsilon: 0.488 | Buffer:  2513 | Max Q: 5.161\n",
      "Episode  123 | Reward:   9.00 | Steps: 2522 | Avg Loss: 1.1861 | Epsilon: 0.486 | Buffer:  2522 | Max Q: 5.132\n",
      "Episode  124 | Reward:  14.00 | Steps: 2536 | Avg Loss: 1.2421 | Epsilon: 0.483 | Buffer:  2536 | Max Q: 5.311\n",
      "Episode  125 | Reward:  29.00 | Steps: 2565 | Avg Loss: 1.6365 | Epsilon: 0.481 | Buffer:  2565 | Max Q: 3.941\n",
      "Episode  126 | Reward:  11.00 | Steps: 2576 | Avg Loss: 1.3267 | Epsilon: 0.479 | Buffer:  2576 | Max Q: 2.648\n",
      "Episode  127 | Reward:  38.00 | Steps: 2614 | Avg Loss: 1.6073 | Epsilon: 0.476 | Buffer:  2614 | Max Q: 3.059\n",
      "Episode  128 | Reward:  17.00 | Steps: 2631 | Avg Loss: 1.0663 | Epsilon: 0.474 | Buffer:  2631 | Max Q: 8.523\n",
      "Episode  129 | Reward:  10.00 | Steps: 2641 | Avg Loss: 1.1627 | Epsilon: 0.471 | Buffer:  2641 | Max Q: 2.858\n",
      "Episode  130 | Reward:  11.00 | Steps: 2652 | Avg Loss: 1.0253 | Epsilon: 0.469 | Buffer:  2652 | Max Q: 3.348\n",
      "Episode  131 | Reward:  10.00 | Steps: 2662 | Avg Loss: 1.6079 | Epsilon: 0.467 | Buffer:  2662 | Max Q: 5.408\n",
      "Episode  132 | Reward:  12.00 | Steps: 2674 | Avg Loss: 2.1206 | Epsilon: 0.464 | Buffer:  2674 | Max Q: 3.507\n",
      "Episode  133 | Reward:  23.00 | Steps: 2697 | Avg Loss: 1.5354 | Epsilon: 0.462 | Buffer:  2697 | Max Q: 4.082\n",
      "Episode  134 | Reward:  19.00 | Steps: 2716 | Avg Loss: 1.4317 | Epsilon: 0.460 | Buffer:  2716 | Max Q: 1.991\n",
      "Episode  135 | Reward:   9.00 | Steps: 2725 | Avg Loss: 1.2600 | Epsilon: 0.457 | Buffer:  2725 | Max Q: 7.316\n",
      "Episode  136 | Reward:  16.00 | Steps: 2741 | Avg Loss: 1.3257 | Epsilon: 0.455 | Buffer:  2741 | Max Q: 5.140\n",
      "\n",
      "--- Episode 137: Updated target network. ---\n",
      "Episode  137 | Reward:   9.00 | Steps: 2750 | Avg Loss: 1.4587 | Epsilon: 0.453 | Buffer:  2750 | Max Q: 3.876\n",
      "Episode  138 | Reward:   9.00 | Steps: 2759 | Avg Loss: 1.4887 | Epsilon: 0.451 | Buffer:  2759 | Max Q: 4.690\n",
      "Episode  139 | Reward:  19.00 | Steps: 2778 | Avg Loss: 1.8561 | Epsilon: 0.448 | Buffer:  2778 | Max Q: 3.613\n",
      "Episode  140 | Reward:  10.00 | Steps: 2788 | Avg Loss: 1.6184 | Epsilon: 0.446 | Buffer:  2788 | Max Q: 4.270\n",
      "Episode  141 | Reward:  16.00 | Steps: 2804 | Avg Loss: 1.8753 | Epsilon: 0.444 | Buffer:  2804 | Max Q: 9.517\n",
      "Episode  142 | Reward:  12.00 | Steps: 2816 | Avg Loss: 1.5391 | Epsilon: 0.442 | Buffer:  2816 | Max Q: 3.020\n",
      "Episode  143 | Reward:  12.00 | Steps: 2828 | Avg Loss: 1.7932 | Epsilon: 0.439 | Buffer:  2828 | Max Q: 9.288\n",
      "Episode  144 | Reward:  11.00 | Steps: 2839 | Avg Loss: 1.7993 | Epsilon: 0.437 | Buffer:  2839 | Max Q: 7.013\n",
      "Episode  145 | Reward:  29.00 | Steps: 2868 | Avg Loss: 1.7552 | Epsilon: 0.435 | Buffer:  2868 | Max Q: 10.914\n",
      "Episode  146 | Reward:  27.00 | Steps: 2895 | Avg Loss: 1.8304 | Epsilon: 0.433 | Buffer:  2895 | Max Q: 3.282\n",
      "Episode  147 | Reward:  15.00 | Steps: 2910 | Avg Loss: 1.3914 | Epsilon: 0.431 | Buffer:  2910 | Max Q: 4.382\n",
      "Episode  148 | Reward:  11.00 | Steps: 2921 | Avg Loss: 1.5900 | Epsilon: 0.429 | Buffer:  2921 | Max Q: 3.302\n",
      "Episode  149 | Reward:  10.00 | Steps: 2931 | Avg Loss: 1.5219 | Epsilon: 0.426 | Buffer:  2931 | Max Q: 2.916\n",
      "Episode  150 | Reward:  13.00 | Steps: 2944 | Avg Loss: 1.2628 | Epsilon: 0.424 | Buffer:  2944 | Max Q: 3.214\n",
      "Episode  151 | Reward:  23.00 | Steps: 2967 | Avg Loss: 1.7607 | Epsilon: 0.422 | Buffer:  2967 | Max Q: 4.159\n",
      "Episode  152 | Reward:  13.00 | Steps: 2980 | Avg Loss: 1.0357 | Epsilon: 0.420 | Buffer:  2980 | Max Q: 4.329\n",
      "Episode  153 | Reward:  13.00 | Steps: 2993 | Avg Loss: 1.7437 | Epsilon: 0.418 | Buffer:  2993 | Max Q: 3.634\n",
      "\n",
      "--- Episode 154: Updated target network. ---\n",
      "Episode  154 | Reward:  24.00 | Steps: 3017 | Avg Loss: 1.6061 | Epsilon: 0.416 | Buffer:  3017 | Max Q: 1.905\n",
      "Episode  155 | Reward:  12.00 | Steps: 3029 | Avg Loss: 1.3962 | Epsilon: 0.414 | Buffer:  3029 | Max Q: 4.210\n",
      "Episode  156 | Reward:  14.00 | Steps: 3043 | Avg Loss: 1.5451 | Epsilon: 0.412 | Buffer:  3043 | Max Q: 3.079\n",
      "Episode  157 | Reward:  14.00 | Steps: 3057 | Avg Loss: 1.5463 | Epsilon: 0.410 | Buffer:  3057 | Max Q: 2.367\n",
      "Episode  158 | Reward:  22.00 | Steps: 3079 | Avg Loss: 2.0478 | Epsilon: 0.408 | Buffer:  3079 | Max Q: 3.613\n",
      "Episode  159 | Reward:  11.00 | Steps: 3090 | Avg Loss: 1.9068 | Epsilon: 0.406 | Buffer:  3090 | Max Q: 3.144\n",
      "Episode  160 | Reward:  13.00 | Steps: 3103 | Avg Loss: 2.0873 | Epsilon: 0.404 | Buffer:  3103 | Max Q: 3.259\n",
      "Episode  161 | Reward:   9.00 | Steps: 3112 | Avg Loss: 1.9678 | Epsilon: 0.402 | Buffer:  3112 | Max Q: 3.128\n",
      "Episode  162 | Reward:  23.00 | Steps: 3135 | Avg Loss: 1.6755 | Epsilon: 0.400 | Buffer:  3135 | Max Q: 9.551\n",
      "Episode  163 | Reward:  17.00 | Steps: 3152 | Avg Loss: 1.6773 | Epsilon: 0.398 | Buffer:  3152 | Max Q: 4.661\n",
      "Episode  164 | Reward:  41.00 | Steps: 3193 | Avg Loss: 1.6656 | Epsilon: 0.396 | Buffer:  3193 | Max Q: 12.198\n",
      "Episode  165 | Reward:   9.00 | Steps: 3202 | Avg Loss: 1.8088 | Epsilon: 0.394 | Buffer:  3202 | Max Q: 3.083\n",
      "Episode  166 | Reward:  12.00 | Steps: 3214 | Avg Loss: 1.5989 | Epsilon: 0.392 | Buffer:  3214 | Max Q: 3.821\n",
      "\n",
      "--- Episode 167: Updated target network. ---\n",
      "Episode  167 | Reward:  36.00 | Steps: 3250 | Avg Loss: 1.4773 | Epsilon: 0.390 | Buffer:  3250 | Max Q: 3.639\n",
      "Episode  168 | Reward:  17.00 | Steps: 3267 | Avg Loss: 2.1711 | Epsilon: 0.388 | Buffer:  3267 | Max Q: 3.094\n",
      "Episode  169 | Reward:  11.00 | Steps: 3278 | Avg Loss: 1.9434 | Epsilon: 0.386 | Buffer:  3278 | Max Q: 3.492\n",
      "Episode  170 | Reward:  14.00 | Steps: 3292 | Avg Loss: 2.5981 | Epsilon: 0.384 | Buffer:  3292 | Max Q: 3.010\n",
      "Episode  171 | Reward:  10.00 | Steps: 3302 | Avg Loss: 2.5262 | Epsilon: 0.382 | Buffer:  3302 | Max Q: 2.375\n",
      "Episode  172 | Reward:   9.00 | Steps: 3311 | Avg Loss: 2.2060 | Epsilon: 0.380 | Buffer:  3311 | Max Q: 2.773\n",
      "Episode  173 | Reward:  12.00 | Steps: 3323 | Avg Loss: 2.9153 | Epsilon: 0.378 | Buffer:  3323 | Max Q: 1.731\n",
      "Episode  174 | Reward:  15.00 | Steps: 3338 | Avg Loss: 2.4131 | Epsilon: 0.376 | Buffer:  3338 | Max Q: 3.482\n",
      "Episode  175 | Reward:  27.00 | Steps: 3365 | Avg Loss: 2.0421 | Epsilon: 0.374 | Buffer:  3365 | Max Q: 3.546\n",
      "Episode  176 | Reward:  15.00 | Steps: 3380 | Avg Loss: 1.6836 | Epsilon: 0.372 | Buffer:  3380 | Max Q: 3.230\n",
      "Episode  177 | Reward:  24.00 | Steps: 3404 | Avg Loss: 2.0128 | Epsilon: 0.371 | Buffer:  3404 | Max Q: 3.474\n",
      "Episode  178 | Reward:  19.00 | Steps: 3423 | Avg Loss: 1.8839 | Epsilon: 0.369 | Buffer:  3423 | Max Q: 3.069\n",
      "Episode  179 | Reward:  47.00 | Steps: 3470 | Avg Loss: 2.0779 | Epsilon: 0.367 | Buffer:  3470 | Max Q: 7.455\n",
      "Episode  180 | Reward:  20.00 | Steps: 3490 | Avg Loss: 1.8609 | Epsilon: 0.365 | Buffer:  3490 | Max Q: 2.184\n",
      "\n",
      "--- Episode 181: Updated target network. ---\n",
      "Episode  181 | Reward:  11.00 | Steps: 3501 | Avg Loss: 1.8069 | Epsilon: 0.363 | Buffer:  3501 | Max Q: 3.071\n",
      "Episode  182 | Reward:  31.00 | Steps: 3532 | Avg Loss: 2.2158 | Epsilon: 0.361 | Buffer:  3532 | Max Q: 5.973\n",
      "Episode  183 | Reward:  12.00 | Steps: 3544 | Avg Loss: 1.9918 | Epsilon: 0.360 | Buffer:  3544 | Max Q: 4.433\n",
      "Episode  184 | Reward:  20.00 | Steps: 3564 | Avg Loss: 2.0997 | Epsilon: 0.358 | Buffer:  3564 | Max Q: 1.506\n",
      "Episode  185 | Reward:  14.00 | Steps: 3578 | Avg Loss: 2.1543 | Epsilon: 0.356 | Buffer:  3578 | Max Q: 2.642\n",
      "Episode  186 | Reward:  11.00 | Steps: 3589 | Avg Loss: 1.8979 | Epsilon: 0.354 | Buffer:  3589 | Max Q: 1.589\n",
      "Episode  187 | Reward:  11.00 | Steps: 3600 | Avg Loss: 1.4984 | Epsilon: 0.352 | Buffer:  3600 | Max Q: 2.460\n",
      "Episode  188 | Reward:  33.00 | Steps: 3633 | Avg Loss: 1.9409 | Epsilon: 0.351 | Buffer:  3633 | Max Q: 3.501\n",
      "Episode  189 | Reward:  11.00 | Steps: 3644 | Avg Loss: 1.7459 | Epsilon: 0.349 | Buffer:  3644 | Max Q: 3.686\n",
      "Episode  190 | Reward:  14.00 | Steps: 3658 | Avg Loss: 1.8371 | Epsilon: 0.347 | Buffer:  3658 | Max Q: 2.318\n",
      "Episode  191 | Reward:  15.00 | Steps: 3673 | Avg Loss: 1.8902 | Epsilon: 0.346 | Buffer:  3673 | Max Q: 4.993\n",
      "Episode  192 | Reward:  16.00 | Steps: 3689 | Avg Loss: 2.0067 | Epsilon: 0.344 | Buffer:  3689 | Max Q: 3.189\n",
      "Episode  193 | Reward:  14.00 | Steps: 3703 | Avg Loss: 1.7280 | Epsilon: 0.342 | Buffer:  3703 | Max Q: 2.548\n",
      "Episode  194 | Reward:  17.00 | Steps: 3720 | Avg Loss: 2.0278 | Epsilon: 0.340 | Buffer:  3720 | Max Q: 2.200\n",
      "\n",
      "--- Episode 195: Updated target network. ---\n",
      "Episode  195 | Reward:  33.00 | Steps: 3753 | Avg Loss: 2.2063 | Epsilon: 0.339 | Buffer:  3753 | Max Q: 6.385\n",
      "Episode  196 | Reward:  28.00 | Steps: 3781 | Avg Loss: 2.7871 | Epsilon: 0.337 | Buffer:  3781 | Max Q: 6.205\n",
      "Episode  197 | Reward:  19.00 | Steps: 3800 | Avg Loss: 2.3284 | Epsilon: 0.335 | Buffer:  3800 | Max Q: 1.490\n",
      "Episode  198 | Reward:  26.00 | Steps: 3826 | Avg Loss: 2.3755 | Epsilon: 0.334 | Buffer:  3826 | Max Q: 3.560\n",
      "Episode  199 | Reward:  18.00 | Steps: 3844 | Avg Loss: 2.5148 | Epsilon: 0.332 | Buffer:  3844 | Max Q: 4.051\n",
      "Saved checkpoint at episode 200\n",
      "Episode  200 | Reward:  21.00 | Steps: 3865 | Avg Loss: 2.1512 | Epsilon: 0.330 | Buffer:  3865 | Max Q: 4.446\n",
      "Episode  201 | Reward:  31.00 | Steps: 3896 | Avg Loss: 2.0447 | Epsilon: 0.329 | Buffer:  3896 | Max Q: 5.107\n",
      "Episode  202 | Reward:  40.00 | Steps: 3936 | Avg Loss: 2.1274 | Epsilon: 0.327 | Buffer:  3936 | Max Q: 5.919\n",
      "Episode  203 | Reward:  15.00 | Steps: 3951 | Avg Loss: 2.6136 | Epsilon: 0.325 | Buffer:  3951 | Max Q: 2.950\n",
      "Episode  204 | Reward:  14.00 | Steps: 3965 | Avg Loss: 1.9666 | Epsilon: 0.324 | Buffer:  3965 | Max Q: 2.471\n",
      "Episode  205 | Reward:  12.00 | Steps: 3977 | Avg Loss: 1.4934 | Epsilon: 0.322 | Buffer:  3977 | Max Q: 5.771\n",
      "Episode  206 | Reward:  22.00 | Steps: 3999 | Avg Loss: 1.8032 | Epsilon: 0.320 | Buffer:  3999 | Max Q: 2.835\n",
      "\n",
      "--- Episode 207: Updated target network. ---\n",
      "Episode  207 | Reward:  33.00 | Steps: 4032 | Avg Loss: 2.3880 | Epsilon: 0.319 | Buffer:  4032 | Max Q: 0.841\n",
      "Episode  208 | Reward:  12.00 | Steps: 4044 | Avg Loss: 2.3128 | Epsilon: 0.317 | Buffer:  4044 | Max Q: 4.161\n",
      "Episode  209 | Reward:  13.00 | Steps: 4057 | Avg Loss: 2.1677 | Epsilon: 0.316 | Buffer:  4057 | Max Q: 1.238\n",
      "Episode  210 | Reward:  39.00 | Steps: 4096 | Avg Loss: 2.2277 | Epsilon: 0.314 | Buffer:  4096 | Max Q: 9.283\n",
      "Episode  211 | Reward:  13.00 | Steps: 4109 | Avg Loss: 2.8377 | Epsilon: 0.313 | Buffer:  4109 | Max Q: 3.733\n",
      "Episode  212 | Reward:  29.00 | Steps: 4138 | Avg Loss: 2.4241 | Epsilon: 0.311 | Buffer:  4138 | Max Q: 4.627\n",
      "Episode  213 | Reward:  49.00 | Steps: 4187 | Avg Loss: 2.2077 | Epsilon: 0.309 | Buffer:  4187 | Max Q: 5.541\n",
      "Episode  214 | Reward:  45.00 | Steps: 4232 | Avg Loss: 2.2013 | Epsilon: 0.308 | Buffer:  4232 | Max Q: 5.733\n",
      "Episode  215 | Reward:  11.00 | Steps: 4243 | Avg Loss: 2.2459 | Epsilon: 0.306 | Buffer:  4243 | Max Q: 3.922\n",
      "\n",
      "--- Episode 216: Updated target network. ---\n",
      "Episode  216 | Reward:  27.00 | Steps: 4270 | Avg Loss: 2.6271 | Epsilon: 0.305 | Buffer:  4270 | Max Q: 6.814\n",
      "Episode  217 | Reward:  27.00 | Steps: 4297 | Avg Loss: 2.5553 | Epsilon: 0.303 | Buffer:  4297 | Max Q: 3.409\n",
      "Episode  218 | Reward:  30.00 | Steps: 4327 | Avg Loss: 2.4393 | Epsilon: 0.302 | Buffer:  4327 | Max Q: 10.926\n",
      "Episode  219 | Reward:  48.00 | Steps: 4375 | Avg Loss: 2.3795 | Epsilon: 0.300 | Buffer:  4375 | Max Q: 11.300\n",
      "Episode  220 | Reward:  39.00 | Steps: 4414 | Avg Loss: 2.3694 | Epsilon: 0.299 | Buffer:  4414 | Max Q: 8.446\n",
      "Episode  221 | Reward:  35.00 | Steps: 4449 | Avg Loss: 2.3673 | Epsilon: 0.297 | Buffer:  4449 | Max Q: 3.636\n",
      "\n",
      "--- Episode 222: Updated target network. ---\n",
      "Episode  222 | Reward:  52.00 | Steps: 4501 | Avg Loss: 2.4376 | Epsilon: 0.296 | Buffer:  4501 | Max Q: 2.792\n",
      "Episode  223 | Reward:  19.00 | Steps: 4520 | Avg Loss: 2.5081 | Epsilon: 0.294 | Buffer:  4520 | Max Q: 2.413\n",
      "Episode  224 | Reward:  36.00 | Steps: 4556 | Avg Loss: 2.6250 | Epsilon: 0.293 | Buffer:  4556 | Max Q: 22.089\n",
      "Episode  225 | Reward:  14.00 | Steps: 4570 | Avg Loss: 2.5106 | Epsilon: 0.291 | Buffer:  4570 | Max Q: 2.163\n",
      "Episode  226 | Reward:  54.00 | Steps: 4624 | Avg Loss: 2.8930 | Epsilon: 0.290 | Buffer:  4624 | Max Q: 25.979\n",
      "Episode  227 | Reward:  50.00 | Steps: 4674 | Avg Loss: 3.0043 | Epsilon: 0.288 | Buffer:  4674 | Max Q: 18.743\n",
      "Episode  228 | Reward:  36.00 | Steps: 4710 | Avg Loss: 2.3102 | Epsilon: 0.287 | Buffer:  4710 | Max Q: 6.830\n",
      "Episode  229 | Reward:  22.00 | Steps: 4732 | Avg Loss: 3.0324 | Epsilon: 0.286 | Buffer:  4732 | Max Q: 4.647\n",
      "\n",
      "--- Episode 230: Updated target network. ---\n",
      "Episode  230 | Reward:  34.00 | Steps: 4766 | Avg Loss: 2.3931 | Epsilon: 0.284 | Buffer:  4766 | Max Q: 4.240\n",
      "Episode  231 | Reward:  20.00 | Steps: 4786 | Avg Loss: 3.0274 | Epsilon: 0.283 | Buffer:  4786 | Max Q: 6.486\n",
      "Episode  232 | Reward:  43.00 | Steps: 4829 | Avg Loss: 2.8037 | Epsilon: 0.281 | Buffer:  4829 | Max Q: 16.409\n",
      "Episode  233 | Reward:  54.00 | Steps: 4883 | Avg Loss: 2.6356 | Epsilon: 0.280 | Buffer:  4883 | Max Q: 19.839\n",
      "Episode  234 | Reward:  20.00 | Steps: 4903 | Avg Loss: 2.9346 | Epsilon: 0.279 | Buffer:  4903 | Max Q: 3.462\n",
      "Episode  235 | Reward:  50.00 | Steps: 4953 | Avg Loss: 2.9758 | Epsilon: 0.277 | Buffer:  4953 | Max Q: 13.843\n",
      "Episode  236 | Reward:  28.00 | Steps: 4981 | Avg Loss: 3.1983 | Epsilon: 0.276 | Buffer:  4981 | Max Q: 15.025\n",
      "\n",
      "--- Episode 237: Updated target network. ---\n",
      "Episode  237 | Reward:  33.00 | Steps: 5014 | Avg Loss: 3.0573 | Epsilon: 0.274 | Buffer:  5014 | Max Q: 7.173\n",
      "Episode  238 | Reward:  56.00 | Steps: 5070 | Avg Loss: 3.6869 | Epsilon: 0.273 | Buffer:  5070 | Max Q: 12.425\n",
      "Episode  239 | Reward:  52.00 | Steps: 5122 | Avg Loss: 3.4306 | Epsilon: 0.272 | Buffer:  5122 | Max Q: 2.149\n",
      "Episode  240 | Reward:  15.00 | Steps: 5137 | Avg Loss: 2.8309 | Epsilon: 0.270 | Buffer:  5137 | Max Q: 9.128\n",
      "Episode  241 | Reward:  45.00 | Steps: 5182 | Avg Loss: 3.3955 | Epsilon: 0.269 | Buffer:  5182 | Max Q: 11.757\n",
      "Episode  242 | Reward:  29.00 | Steps: 5211 | Avg Loss: 2.9083 | Epsilon: 0.268 | Buffer:  5211 | Max Q: 6.385\n",
      "Episode  243 | Reward:  37.00 | Steps: 5248 | Avg Loss: 2.8814 | Epsilon: 0.266 | Buffer:  5248 | Max Q: 5.251\n",
      "\n",
      "--- Episode 244: Updated target network. ---\n",
      "Episode  244 | Reward:  39.00 | Steps: 5287 | Avg Loss: 4.1816 | Epsilon: 0.265 | Buffer:  5287 | Max Q: 6.824\n",
      "Episode  245 | Reward:  27.00 | Steps: 5314 | Avg Loss: 3.9683 | Epsilon: 0.264 | Buffer:  5314 | Max Q: 6.071\n",
      "Episode  246 | Reward:  75.00 | Steps: 5389 | Avg Loss: 3.6727 | Epsilon: 0.262 | Buffer:  5389 | Max Q: 5.660\n",
      "Episode  247 | Reward:  30.00 | Steps: 5419 | Avg Loss: 3.4395 | Epsilon: 0.261 | Buffer:  5419 | Max Q: 10.950\n",
      "Episode  248 | Reward:  32.00 | Steps: 5451 | Avg Loss: 3.6064 | Epsilon: 0.260 | Buffer:  5451 | Max Q: 10.851\n",
      "Episode  249 | Reward:  37.00 | Steps: 5488 | Avg Loss: 3.6477 | Epsilon: 0.258 | Buffer:  5488 | Max Q: 5.903\n",
      "\n",
      "--- Episode 250: Updated target network. ---\n",
      "Episode  250 | Reward:  45.00 | Steps: 5533 | Avg Loss: 3.9392 | Epsilon: 0.257 | Buffer:  5533 | Max Q: 0.830\n",
      "Episode  251 | Reward:  32.00 | Steps: 5565 | Avg Loss: 3.8919 | Epsilon: 0.256 | Buffer:  5565 | Max Q: 1.928\n",
      "Episode  252 | Reward:  45.00 | Steps: 5610 | Avg Loss: 3.9896 | Epsilon: 0.254 | Buffer:  5610 | Max Q: 22.636\n",
      "Episode  253 | Reward:  36.00 | Steps: 5646 | Avg Loss: 4.0243 | Epsilon: 0.253 | Buffer:  5646 | Max Q: 21.333\n",
      "Episode  254 | Reward:  26.00 | Steps: 5672 | Avg Loss: 3.5211 | Epsilon: 0.252 | Buffer:  5672 | Max Q: 3.659\n",
      "Episode  255 | Reward:  29.00 | Steps: 5701 | Avg Loss: 4.0261 | Epsilon: 0.251 | Buffer:  5701 | Max Q: 3.605\n",
      "\n",
      "--- Episode 256: Updated target network. ---\n",
      "Episode  256 | Reward:  63.00 | Steps: 5764 | Avg Loss: 4.3553 | Epsilon: 0.249 | Buffer:  5764 | Max Q: 18.635\n",
      "Episode  257 | Reward:  56.00 | Steps: 5820 | Avg Loss: 3.8152 | Epsilon: 0.248 | Buffer:  5820 | Max Q: 14.452\n",
      "Episode  258 | Reward:  14.00 | Steps: 5834 | Avg Loss: 3.7864 | Epsilon: 0.247 | Buffer:  5834 | Max Q: 2.119\n",
      "Episode  259 | Reward:  39.00 | Steps: 5873 | Avg Loss: 4.0456 | Epsilon: 0.246 | Buffer:  5873 | Max Q: 11.055\n",
      "Episode  260 | Reward:  40.00 | Steps: 5913 | Avg Loss: 3.8372 | Epsilon: 0.244 | Buffer:  5913 | Max Q: 14.873\n",
      "Episode  261 | Reward:  53.00 | Steps: 5966 | Avg Loss: 3.8671 | Epsilon: 0.243 | Buffer:  5966 | Max Q: 12.568\n",
      "\n",
      "--- Episode 262: Updated target network. ---\n",
      "Episode  262 | Reward:  55.00 | Steps: 6021 | Avg Loss: 3.6967 | Epsilon: 0.242 | Buffer:  6021 | Max Q: 8.697\n",
      "Episode  263 | Reward:  41.00 | Steps: 6062 | Avg Loss: 4.5296 | Epsilon: 0.241 | Buffer:  6062 | Max Q: 0.262\n",
      "Episode  264 | Reward:  65.00 | Steps: 6127 | Avg Loss: 3.9727 | Epsilon: 0.240 | Buffer:  6127 | Max Q: 4.106\n",
      "Episode  265 | Reward:  21.00 | Steps: 6148 | Avg Loss: 3.9038 | Epsilon: 0.238 | Buffer:  6148 | Max Q: 11.918\n",
      "Episode  266 | Reward:  35.00 | Steps: 6183 | Avg Loss: 3.8889 | Epsilon: 0.237 | Buffer:  6183 | Max Q: 7.210\n",
      "\n",
      "--- Episode 267: Updated target network. ---\n",
      "Episode  267 | Reward:  71.00 | Steps: 6254 | Avg Loss: 3.9279 | Epsilon: 0.236 | Buffer:  6254 | Max Q: 10.829\n",
      "Episode  268 | Reward:  62.00 | Steps: 6316 | Avg Loss: 4.0496 | Epsilon: 0.235 | Buffer:  6316 | Max Q: 3.106\n",
      "Episode  269 | Reward:  38.00 | Steps: 6354 | Avg Loss: 3.9067 | Epsilon: 0.234 | Buffer:  6354 | Max Q: 8.000\n",
      "Episode  270 | Reward:  29.00 | Steps: 6383 | Avg Loss: 3.8497 | Epsilon: 0.233 | Buffer:  6383 | Max Q: 1.792\n",
      "Episode  271 | Reward:  21.00 | Steps: 6404 | Avg Loss: 3.8917 | Epsilon: 0.231 | Buffer:  6404 | Max Q: 4.946\n",
      "Episode  272 | Reward:  22.00 | Steps: 6426 | Avg Loss: 4.4228 | Epsilon: 0.230 | Buffer:  6426 | Max Q: 11.522\n",
      "Episode  273 | Reward:  70.00 | Steps: 6496 | Avg Loss: 4.0180 | Epsilon: 0.229 | Buffer:  6496 | Max Q: 5.821\n",
      "\n",
      "--- Episode 274: Updated target network. ---\n",
      "Episode  274 | Reward:  34.00 | Steps: 6530 | Avg Loss: 4.1276 | Epsilon: 0.228 | Buffer:  6530 | Max Q: 4.147\n",
      "Episode  275 | Reward:  58.00 | Steps: 6588 | Avg Loss: 4.1499 | Epsilon: 0.227 | Buffer:  6588 | Max Q: 4.353\n",
      "Episode  276 | Reward:  26.00 | Steps: 6614 | Avg Loss: 4.1911 | Epsilon: 0.226 | Buffer:  6614 | Max Q: 9.247\n",
      "Episode  277 | Reward:  80.00 | Steps: 6694 | Avg Loss: 3.8942 | Epsilon: 0.225 | Buffer:  6694 | Max Q: 10.546\n",
      "Episode  278 | Reward:  39.00 | Steps: 6733 | Avg Loss: 3.8030 | Epsilon: 0.223 | Buffer:  6733 | Max Q: 8.623\n",
      "\n",
      "--- Episode 279: Updated target network. ---\n",
      "Episode  279 | Reward:  28.00 | Steps: 6761 | Avg Loss: 4.1456 | Epsilon: 0.222 | Buffer:  6761 | Max Q: 9.397\n",
      "Episode  280 | Reward:  58.00 | Steps: 6819 | Avg Loss: 4.4391 | Epsilon: 0.221 | Buffer:  6819 | Max Q: 6.350\n",
      "Episode  281 | Reward:  48.00 | Steps: 6867 | Avg Loss: 4.3467 | Epsilon: 0.220 | Buffer:  6867 | Max Q: 4.032\n",
      "Episode  282 | Reward:  50.00 | Steps: 6917 | Avg Loss: 3.8938 | Epsilon: 0.219 | Buffer:  6917 | Max Q: 4.477\n",
      "Episode  283 | Reward:  32.00 | Steps: 6949 | Avg Loss: 4.1307 | Epsilon: 0.218 | Buffer:  6949 | Max Q: 2.568\n",
      "\n",
      "--- Episode 284: Updated target network. ---\n",
      "Episode  284 | Reward:  53.00 | Steps: 7002 | Avg Loss: 4.1213 | Epsilon: 0.217 | Buffer:  7002 | Max Q: -0.765\n",
      "Episode  285 | Reward:  58.00 | Steps: 7060 | Avg Loss: 4.1592 | Epsilon: 0.216 | Buffer:  7060 | Max Q: 13.598\n",
      "Episode  286 | Reward:  33.00 | Steps: 7093 | Avg Loss: 4.5501 | Epsilon: 0.215 | Buffer:  7093 | Max Q: -1.662\n",
      "Episode  287 | Reward:  74.00 | Steps: 7167 | Avg Loss: 4.3404 | Epsilon: 0.214 | Buffer:  7167 | Max Q: 1.037\n",
      "Episode  288 | Reward:  24.00 | Steps: 7191 | Avg Loss: 5.0055 | Epsilon: 0.212 | Buffer:  7191 | Max Q: 2.354\n",
      "Episode  289 | Reward:  45.00 | Steps: 7236 | Avg Loss: 4.6021 | Epsilon: 0.211 | Buffer:  7236 | Max Q: 1.358\n",
      "\n",
      "--- Episode 290: Updated target network. ---\n",
      "Episode  290 | Reward:  75.00 | Steps: 7311 | Avg Loss: 4.6042 | Epsilon: 0.210 | Buffer:  7311 | Max Q: 2.723\n",
      "Episode  291 | Reward:  39.00 | Steps: 7350 | Avg Loss: 31.3838 | Epsilon: 0.209 | Buffer:  7350 | Max Q: 9.670\n",
      "Episode  292 | Reward:  28.00 | Steps: 7378 | Avg Loss: 30.9513 | Epsilon: 0.208 | Buffer:  7378 | Max Q: 10.010\n",
      "Episode  293 | Reward:  28.00 | Steps: 7406 | Avg Loss: 30.2679 | Epsilon: 0.207 | Buffer:  7406 | Max Q: 12.947\n",
      "Episode  294 | Reward:  28.00 | Steps: 7434 | Avg Loss: 28.4208 | Epsilon: 0.206 | Buffer:  7434 | Max Q: 5.531\n",
      "Episode  295 | Reward:  10.00 | Steps: 7444 | Avg Loss: 24.7547 | Epsilon: 0.205 | Buffer:  7444 | Max Q: 13.643\n",
      "Episode  296 | Reward:   9.00 | Steps: 7453 | Avg Loss: 27.4058 | Epsilon: 0.204 | Buffer:  7453 | Max Q: 13.198\n",
      "Episode  297 | Reward:  34.00 | Steps: 7487 | Avg Loss: 29.4078 | Epsilon: 0.203 | Buffer:  7487 | Max Q: 21.256\n",
      "\n",
      "--- Episode 298: Updated target network. ---\n",
      "Episode  298 | Reward:  28.00 | Steps: 7515 | Avg Loss: 16.9849 | Epsilon: 0.202 | Buffer:  7515 | Max Q: 4.597\n",
      "Episode  299 | Reward:  17.00 | Steps: 7532 | Avg Loss: 5.5684 | Epsilon: 0.201 | Buffer:  7532 | Max Q: 7.718\n",
      "Saved checkpoint at episode 300\n",
      "Episode  300 | Reward:  15.00 | Steps: 7547 | Avg Loss: 6.6532 | Epsilon: 0.200 | Buffer:  7547 | Max Q: 14.438\n",
      "Episode  301 | Reward:  47.00 | Steps: 7594 | Avg Loss: 5.3316 | Epsilon: 0.199 | Buffer:  7594 | Max Q: 20.729\n",
      "Episode  302 | Reward:  30.00 | Steps: 7624 | Avg Loss: 6.1184 | Epsilon: 0.198 | Buffer:  7624 | Max Q: 21.453\n",
      "Episode  303 | Reward:  39.00 | Steps: 7663 | Avg Loss: 4.7937 | Epsilon: 0.197 | Buffer:  7663 | Max Q: 21.824\n",
      "Episode  304 | Reward:  21.00 | Steps: 7684 | Avg Loss: 5.9236 | Epsilon: 0.196 | Buffer:  7684 | Max Q: 7.652\n",
      "Episode  305 | Reward:  42.00 | Steps: 7726 | Avg Loss: 5.4587 | Epsilon: 0.195 | Buffer:  7726 | Max Q: 7.519\n",
      "Episode  306 | Reward:  11.00 | Steps: 7737 | Avg Loss: 7.1856 | Epsilon: 0.194 | Buffer:  7737 | Max Q: 9.463\n",
      "\n",
      "--- Episode 307: Updated target network. ---\n",
      "Episode  307 | Reward:  21.00 | Steps: 7758 | Avg Loss: 6.2737 | Epsilon: 0.193 | Buffer:  7758 | Max Q: 17.815\n",
      "Episode  308 | Reward:  26.00 | Steps: 7784 | Avg Loss: 6.1672 | Epsilon: 0.192 | Buffer:  7784 | Max Q: 6.475\n",
      "Episode  309 | Reward:  16.00 | Steps: 7800 | Avg Loss: 5.8603 | Epsilon: 0.191 | Buffer:  7800 | Max Q: 11.986\n",
      "Episode  310 | Reward:  13.00 | Steps: 7813 | Avg Loss: 6.7127 | Epsilon: 0.190 | Buffer:  7813 | Max Q: 14.532\n",
      "Episode  311 | Reward:  10.00 | Steps: 7823 | Avg Loss: 7.8758 | Epsilon: 0.189 | Buffer:  7823 | Max Q: 16.014\n",
      "Episode  312 | Reward:  34.00 | Steps: 7857 | Avg Loss: 5.2911 | Epsilon: 0.188 | Buffer:  7857 | Max Q: 8.762\n",
      "Episode  313 | Reward:  11.00 | Steps: 7868 | Avg Loss: 4.6484 | Epsilon: 0.187 | Buffer:  7868 | Max Q: 10.432\n",
      "Episode  314 | Reward:  33.00 | Steps: 7901 | Avg Loss: 6.0509 | Epsilon: 0.187 | Buffer:  7901 | Max Q: 18.561\n",
      "Episode  315 | Reward:  28.00 | Steps: 7929 | Avg Loss: 5.6967 | Epsilon: 0.186 | Buffer:  7929 | Max Q: 19.498\n",
      "Episode  316 | Reward:   9.00 | Steps: 7938 | Avg Loss: 5.9888 | Epsilon: 0.185 | Buffer:  7938 | Max Q: 10.769\n",
      "Episode  317 | Reward:  28.00 | Steps: 7966 | Avg Loss: 6.1316 | Epsilon: 0.184 | Buffer:  7966 | Max Q: 22.783\n",
      "Episode  318 | Reward:   8.00 | Steps: 7974 | Avg Loss: 7.2687 | Epsilon: 0.183 | Buffer:  7974 | Max Q: 19.841\n",
      "\n",
      "--- Episode 319: Updated target network. ---\n",
      "Episode  319 | Reward:  32.00 | Steps: 8006 | Avg Loss: 6.0901 | Epsilon: 0.182 | Buffer:  8006 | Max Q: 20.418\n",
      "Episode  320 | Reward:  14.00 | Steps: 8020 | Avg Loss: 6.5195 | Epsilon: 0.181 | Buffer:  8020 | Max Q: 14.989\n",
      "Episode  321 | Reward:  44.00 | Steps: 8064 | Avg Loss: 7.1605 | Epsilon: 0.180 | Buffer:  8064 | Max Q: 17.608\n",
      "Episode  322 | Reward:  15.00 | Steps: 8079 | Avg Loss: 6.0424 | Epsilon: 0.179 | Buffer:  8079 | Max Q: 7.292\n",
      "Episode  323 | Reward:  27.00 | Steps: 8106 | Avg Loss: 6.2044 | Epsilon: 0.178 | Buffer:  8106 | Max Q: 20.879\n",
      "Episode  324 | Reward:  39.00 | Steps: 8145 | Avg Loss: 7.4309 | Epsilon: 0.177 | Buffer:  8145 | Max Q: 22.384\n",
      "Episode  325 | Reward:  11.00 | Steps: 8156 | Avg Loss: 7.0338 | Epsilon: 0.176 | Buffer:  8156 | Max Q: 17.341\n",
      "Episode  326 | Reward:  28.00 | Steps: 8184 | Avg Loss: 6.7183 | Epsilon: 0.176 | Buffer:  8184 | Max Q: 21.121\n",
      "Episode  327 | Reward:  33.00 | Steps: 8217 | Avg Loss: 7.0486 | Epsilon: 0.175 | Buffer:  8217 | Max Q: 22.776\n",
      "Episode  328 | Reward:   8.00 | Steps: 8225 | Avg Loss: 5.7323 | Epsilon: 0.174 | Buffer:  8225 | Max Q: 19.204\n",
      "\n",
      "--- Episode 329: Updated target network. ---\n",
      "Episode  329 | Reward:  41.00 | Steps: 8266 | Avg Loss: 8.7167 | Epsilon: 0.173 | Buffer:  8266 | Max Q: 21.636\n",
      "Episode  330 | Reward:  30.00 | Steps: 8296 | Avg Loss: 8.8867 | Epsilon: 0.172 | Buffer:  8296 | Max Q: 23.080\n",
      "Episode  331 | Reward:  17.00 | Steps: 8313 | Avg Loss: 6.7933 | Epsilon: 0.171 | Buffer:  8313 | Max Q: 7.345\n",
      "Episode  332 | Reward:  13.00 | Steps: 8326 | Avg Loss: 8.9030 | Epsilon: 0.170 | Buffer:  8326 | Max Q: 14.969\n",
      "Episode  333 | Reward:  30.00 | Steps: 8356 | Avg Loss: 7.4503 | Epsilon: 0.170 | Buffer:  8356 | Max Q: 22.225\n",
      "Episode  334 | Reward:  30.00 | Steps: 8386 | Avg Loss: 7.4911 | Epsilon: 0.169 | Buffer:  8386 | Max Q: 20.190\n",
      "Episode  335 | Reward:  28.00 | Steps: 8414 | Avg Loss: 7.1103 | Epsilon: 0.168 | Buffer:  8414 | Max Q: 20.668\n",
      "Episode  336 | Reward:  29.00 | Steps: 8443 | Avg Loss: 5.9808 | Epsilon: 0.167 | Buffer:  8443 | Max Q: 21.144\n",
      "Episode  337 | Reward:  40.00 | Steps: 8483 | Avg Loss: 7.0497 | Epsilon: 0.166 | Buffer:  8483 | Max Q: 22.969\n",
      "\n",
      "--- Episode 338: Updated target network. ---\n",
      "Episode  338 | Reward:  44.00 | Steps: 8527 | Avg Loss: 7.3293 | Epsilon: 0.165 | Buffer:  8527 | Max Q: 17.150\n",
      "Episode  339 | Reward:  51.00 | Steps: 8578 | Avg Loss: 9.0614 | Epsilon: 0.165 | Buffer:  8578 | Max Q: 22.957\n",
      "Episode  340 | Reward:  10.00 | Steps: 8588 | Avg Loss: 11.7902 | Epsilon: 0.164 | Buffer:  8588 | Max Q: 4.124\n",
      "Episode  341 | Reward:  35.00 | Steps: 8623 | Avg Loss: 8.1088 | Epsilon: 0.163 | Buffer:  8623 | Max Q: 16.511\n",
      "Episode  342 | Reward:  14.00 | Steps: 8637 | Avg Loss: 8.1040 | Epsilon: 0.162 | Buffer:  8637 | Max Q: 16.764\n",
      "Episode  343 | Reward:  33.00 | Steps: 8670 | Avg Loss: 7.6860 | Epsilon: 0.161 | Buffer:  8670 | Max Q: 8.097\n",
      "Episode  344 | Reward:  26.00 | Steps: 8696 | Avg Loss: 8.0563 | Epsilon: 0.160 | Buffer:  8696 | Max Q: 16.373\n",
      "Episode  345 | Reward:  47.00 | Steps: 8743 | Avg Loss: 7.5219 | Epsilon: 0.160 | Buffer:  8743 | Max Q: 17.386\n",
      "\n",
      "--- Episode 346: Updated target network. ---\n",
      "Episode  346 | Reward:  56.00 | Steps: 8799 | Avg Loss: 8.8795 | Epsilon: 0.159 | Buffer:  8799 | Max Q: 8.359\n",
      "Episode  347 | Reward:  42.00 | Steps: 8841 | Avg Loss: 11.5677 | Epsilon: 0.158 | Buffer:  8841 | Max Q: 24.888\n",
      "Episode  348 | Reward:  25.00 | Steps: 8866 | Avg Loss: 10.3478 | Epsilon: 0.157 | Buffer:  8866 | Max Q: 22.838\n",
      "Episode  349 | Reward:  76.00 | Steps: 8942 | Avg Loss: 8.1960 | Epsilon: 0.156 | Buffer:  8942 | Max Q: 12.583\n",
      "Episode  350 | Reward:  19.00 | Steps: 8961 | Avg Loss: 7.3450 | Epsilon: 0.156 | Buffer:  8961 | Max Q: 18.807\n",
      "\n",
      "--- Episode 351: Updated target network. ---\n",
      "Episode  351 | Reward:  48.00 | Steps: 9009 | Avg Loss: 8.6654 | Epsilon: 0.155 | Buffer:  9009 | Max Q: 5.095\n",
      "Episode  352 | Reward:  35.00 | Steps: 9044 | Avg Loss: 9.2643 | Epsilon: 0.154 | Buffer:  9044 | Max Q: 15.758\n",
      "Episode  353 | Reward:  10.00 | Steps: 9054 | Avg Loss: 10.9544 | Epsilon: 0.153 | Buffer:  9054 | Max Q: 4.898\n",
      "Episode  354 | Reward:  32.00 | Steps: 9086 | Avg Loss: 10.5961 | Epsilon: 0.153 | Buffer:  9086 | Max Q: 22.164\n",
      "Episode  355 | Reward:  41.00 | Steps: 9127 | Avg Loss: 10.1552 | Epsilon: 0.152 | Buffer:  9127 | Max Q: 3.870\n",
      "Episode  356 | Reward:  35.00 | Steps: 9162 | Avg Loss: 7.5962 | Epsilon: 0.151 | Buffer:  9162 | Max Q: 12.263\n",
      "Episode  357 | Reward:  60.00 | Steps: 9222 | Avg Loss: 7.8754 | Epsilon: 0.150 | Buffer:  9222 | Max Q: 25.558\n",
      "\n",
      "--- Episode 358: Updated target network. ---\n",
      "Episode  358 | Reward:  41.00 | Steps: 9263 | Avg Loss: 7.6312 | Epsilon: 0.150 | Buffer:  9263 | Max Q: 18.570\n",
      "Episode  359 | Reward:  56.00 | Steps: 9319 | Avg Loss: 7.6707 | Epsilon: 0.149 | Buffer:  9319 | Max Q: 13.041\n",
      "Episode  360 | Reward:  38.00 | Steps: 9357 | Avg Loss: 9.0047 | Epsilon: 0.148 | Buffer:  9357 | Max Q: 13.231\n",
      "Episode  361 | Reward:  12.00 | Steps: 9369 | Avg Loss: 8.5862 | Epsilon: 0.147 | Buffer:  9369 | Max Q: 19.322\n",
      "Episode  362 | Reward:  77.00 | Steps: 9446 | Avg Loss: 7.5729 | Epsilon: 0.147 | Buffer:  9446 | Max Q: 19.950\n",
      "\n",
      "--- Episode 363: Updated target network. ---\n",
      "Episode  363 | Reward:  60.00 | Steps: 9506 | Avg Loss: 9.5676 | Epsilon: 0.146 | Buffer:  9506 | Max Q: 20.228\n",
      "Episode  364 | Reward:  41.00 | Steps: 9547 | Avg Loss: 8.8050 | Epsilon: 0.145 | Buffer:  9547 | Max Q: 17.404\n",
      "Episode  365 | Reward:  40.00 | Steps: 9587 | Avg Loss: 7.9936 | Epsilon: 0.144 | Buffer:  9587 | Max Q: 0.447\n",
      "Episode  366 | Reward:  47.00 | Steps: 9634 | Avg Loss: 7.4969 | Epsilon: 0.144 | Buffer:  9634 | Max Q: 4.012\n",
      "Episode  367 | Reward:  35.00 | Steps: 9669 | Avg Loss: 6.9344 | Epsilon: 0.143 | Buffer:  9669 | Max Q: 10.107\n",
      "Episode  368 | Reward:  33.00 | Steps: 9702 | Avg Loss: 8.0469 | Epsilon: 0.142 | Buffer:  9702 | Max Q: 6.234\n",
      "Episode  369 | Reward:  42.00 | Steps: 9744 | Avg Loss: 8.0116 | Epsilon: 0.142 | Buffer:  9744 | Max Q: 8.947\n",
      "\n",
      "--- Episode 370: Updated target network. ---\n",
      "Episode  370 | Reward:  66.00 | Steps: 9810 | Avg Loss: 8.6107 | Epsilon: 0.141 | Buffer:  9810 | Max Q: 14.544\n",
      "Episode  371 | Reward:  39.00 | Steps: 9849 | Avg Loss: 8.5504 | Epsilon: 0.140 | Buffer:  9849 | Max Q: 5.023\n",
      "Episode  372 | Reward:  63.00 | Steps: 9912 | Avg Loss: 8.0166 | Epsilon: 0.139 | Buffer:  9912 | Max Q: 7.852\n",
      "Episode  373 | Reward:  46.00 | Steps: 9958 | Avg Loss: 7.8754 | Epsilon: 0.139 | Buffer:  9958 | Max Q: 11.804\n",
      "\n",
      "--- Episode 374: Updated target network. ---\n",
      "Episode  374 | Reward:  61.00 | Steps: 10019 | Avg Loss: 8.2327 | Epsilon: 0.138 | Buffer: 10019 | Max Q: 3.948\n",
      "Episode  375 | Reward:  52.00 | Steps: 10071 | Avg Loss: 6.9017 | Epsilon: 0.137 | Buffer: 10071 | Max Q: 4.863\n",
      "Episode  376 | Reward:  56.00 | Steps: 10127 | Avg Loss: 7.7462 | Epsilon: 0.137 | Buffer: 10127 | Max Q: 15.771\n",
      "Episode  377 | Reward:  94.00 | Steps: 10221 | Avg Loss: 7.0355 | Epsilon: 0.136 | Buffer: 10221 | Max Q: 7.324\n",
      "\n",
      "--- Episode 378: Updated target network. ---\n",
      "Episode  378 | Reward:  77.00 | Steps: 10298 | Avg Loss: 7.7407 | Epsilon: 0.135 | Buffer: 10298 | Max Q: 7.463\n",
      "Episode  379 | Reward:  25.00 | Steps: 10323 | Avg Loss: 8.4118 | Epsilon: 0.135 | Buffer: 10323 | Max Q: 3.106\n",
      "Episode  380 | Reward:  51.00 | Steps: 10374 | Avg Loss: 7.6518 | Epsilon: 0.134 | Buffer: 10374 | Max Q: 8.704\n",
      "Episode  381 | Reward:  27.00 | Steps: 10401 | Avg Loss: 6.9686 | Epsilon: 0.133 | Buffer: 10401 | Max Q: 8.491\n",
      "Episode  382 | Reward:  67.00 | Steps: 10468 | Avg Loss: 8.1229 | Epsilon: 0.133 | Buffer: 10468 | Max Q: 2.060\n",
      "\n",
      "--- Episode 383: Updated target network. ---\n",
      "Episode  383 | Reward:  67.00 | Steps: 10535 | Avg Loss: 8.0221 | Epsilon: 0.132 | Buffer: 10535 | Max Q: 3.697\n",
      "Episode  384 | Reward:  61.00 | Steps: 10596 | Avg Loss: 7.9321 | Epsilon: 0.131 | Buffer: 10596 | Max Q: 5.476\n",
      "Episode  385 | Reward:  68.00 | Steps: 10664 | Avg Loss: 7.4675 | Epsilon: 0.131 | Buffer: 10664 | Max Q: 23.585\n",
      "Episode  386 | Reward:  34.00 | Steps: 10698 | Avg Loss: 7.3708 | Epsilon: 0.130 | Buffer: 10698 | Max Q: 8.206\n",
      "\n",
      "--- Episode 387: Updated target network. ---\n",
      "Episode  387 | Reward:  53.00 | Steps: 10751 | Avg Loss: 7.2509 | Epsilon: 0.129 | Buffer: 10751 | Max Q: 2.968\n",
      "Episode  388 | Reward:  63.00 | Steps: 10814 | Avg Loss: 7.8956 | Epsilon: 0.129 | Buffer: 10814 | Max Q: 9.424\n",
      "Episode  389 | Reward:  52.00 | Steps: 10866 | Avg Loss: 8.4875 | Epsilon: 0.128 | Buffer: 10866 | Max Q: 11.050\n",
      "Episode  390 | Reward:  81.00 | Steps: 10947 | Avg Loss: 7.8261 | Epsilon: 0.127 | Buffer: 10947 | Max Q: 11.288\n",
      "\n",
      "--- Episode 391: Updated target network. ---\n",
      "Episode  391 | Reward:  65.00 | Steps: 11012 | Avg Loss: 7.1349 | Epsilon: 0.127 | Buffer: 11012 | Max Q: 5.100\n",
      "Episode  392 | Reward:  48.00 | Steps: 11060 | Avg Loss: 7.1281 | Epsilon: 0.126 | Buffer: 11060 | Max Q: 8.359\n",
      "Episode  393 | Reward:  44.00 | Steps: 11104 | Avg Loss: 7.9369 | Epsilon: 0.126 | Buffer: 11104 | Max Q: 19.973\n",
      "Episode  394 | Reward:  44.00 | Steps: 11148 | Avg Loss: 7.9258 | Epsilon: 0.125 | Buffer: 11148 | Max Q: 2.705\n",
      "Episode  395 | Reward:  40.00 | Steps: 11188 | Avg Loss: 7.6181 | Epsilon: 0.124 | Buffer: 11188 | Max Q: 7.061\n",
      "Episode  396 | Reward:  53.00 | Steps: 11241 | Avg Loss: 6.4848 | Epsilon: 0.124 | Buffer: 11241 | Max Q: 0.724\n",
      "\n",
      "--- Episode 397: Updated target network. ---\n",
      "Episode  397 | Reward:  41.00 | Steps: 11282 | Avg Loss: 7.0880 | Epsilon: 0.123 | Buffer: 11282 | Max Q: 3.649\n",
      "Episode  398 | Reward:  82.00 | Steps: 11364 | Avg Loss: 7.1789 | Epsilon: 0.122 | Buffer: 11364 | Max Q: 6.696\n",
      "Episode  399 | Reward:  54.00 | Steps: 11418 | Avg Loss: 6.5059 | Epsilon: 0.122 | Buffer: 11418 | Max Q: 18.472\n",
      "Saved checkpoint at episode 400\n",
      "Episode  400 | Reward:  61.00 | Steps: 11479 | Avg Loss: 6.8191 | Epsilon: 0.121 | Buffer: 11479 | Max Q: 8.911\n",
      "\n",
      "--- Episode 401: Updated target network. ---\n",
      "Episode  401 | Reward:  49.00 | Steps: 11528 | Avg Loss: 6.6017 | Epsilon: 0.121 | Buffer: 11528 | Max Q: 4.952\n",
      "Episode  402 | Reward:  66.00 | Steps: 11594 | Avg Loss: 6.4787 | Epsilon: 0.120 | Buffer: 11594 | Max Q: 8.933\n",
      "Episode  403 | Reward:  44.00 | Steps: 11638 | Avg Loss: 6.6045 | Epsilon: 0.119 | Buffer: 11638 | Max Q: 2.530\n",
      "Episode  404 | Reward:  38.00 | Steps: 11676 | Avg Loss: 7.4458 | Epsilon: 0.119 | Buffer: 11676 | Max Q: 10.625\n",
      "Episode  405 | Reward:  66.00 | Steps: 11742 | Avg Loss: 6.2161 | Epsilon: 0.118 | Buffer: 11742 | Max Q: 3.213\n",
      "\n",
      "--- Episode 406: Updated target network. ---\n",
      "Episode  406 | Reward:  45.00 | Steps: 11787 | Avg Loss: 6.2678 | Epsilon: 0.118 | Buffer: 11787 | Max Q: 2.307\n",
      "Episode  407 | Reward:  63.00 | Steps: 11850 | Avg Loss: 6.5081 | Epsilon: 0.117 | Buffer: 11850 | Max Q: 0.647\n",
      "Episode  408 | Reward:  56.00 | Steps: 11906 | Avg Loss: 6.5971 | Epsilon: 0.116 | Buffer: 11906 | Max Q: 3.649\n",
      "Episode  409 | Reward:  63.00 | Steps: 11969 | Avg Loss: 6.4208 | Epsilon: 0.116 | Buffer: 11969 | Max Q: 4.783\n",
      "\n",
      "--- Episode 410: Updated target network. ---\n",
      "Episode  410 | Reward:  53.00 | Steps: 12022 | Avg Loss: 6.1300 | Epsilon: 0.115 | Buffer: 12022 | Max Q: 2.905\n",
      "Episode  411 | Reward:  39.00 | Steps: 12061 | Avg Loss: 6.8528 | Epsilon: 0.115 | Buffer: 12061 | Max Q: 9.302\n",
      "Episode  412 | Reward:  81.00 | Steps: 12142 | Avg Loss: 5.9335 | Epsilon: 0.114 | Buffer: 12142 | Max Q: 6.838\n",
      "Episode  413 | Reward:  98.00 | Steps: 12240 | Avg Loss: 6.3208 | Epsilon: 0.114 | Buffer: 12240 | Max Q: 9.029\n",
      "\n",
      "--- Episode 414: Updated target network. ---\n",
      "Episode  414 | Reward:  88.00 | Steps: 12328 | Avg Loss: 6.9636 | Epsilon: 0.113 | Buffer: 12328 | Max Q: 3.303\n",
      "Episode  415 | Reward:  85.00 | Steps: 12413 | Avg Loss: 6.8605 | Epsilon: 0.112 | Buffer: 12413 | Max Q: 10.243\n",
      "Episode  416 | Reward:  72.00 | Steps: 12485 | Avg Loss: 6.1421 | Epsilon: 0.112 | Buffer: 12485 | Max Q: -1.058\n",
      "\n",
      "--- Episode 417: Updated target network. ---\n",
      "Episode  417 | Reward:  66.00 | Steps: 12551 | Avg Loss: 6.5087 | Epsilon: 0.111 | Buffer: 12551 | Max Q: -0.280\n",
      "Episode  418 | Reward:  69.00 | Steps: 12620 | Avg Loss: 6.1490 | Epsilon: 0.111 | Buffer: 12620 | Max Q: 19.317\n",
      "Episode  419 | Reward:  71.00 | Steps: 12691 | Avg Loss: 6.3201 | Epsilon: 0.110 | Buffer: 12691 | Max Q: 0.802\n",
      "\n",
      "--- Episode 420: Updated target network. ---\n",
      "Episode  420 | Reward:  81.00 | Steps: 12772 | Avg Loss: 6.2401 | Epsilon: 0.110 | Buffer: 12772 | Max Q: 5.851\n",
      "Episode  421 | Reward:  74.00 | Steps: 12846 | Avg Loss: 6.4065 | Epsilon: 0.109 | Buffer: 12846 | Max Q: 4.747\n",
      "Episode  422 | Reward:  63.00 | Steps: 12909 | Avg Loss: 7.0512 | Epsilon: 0.109 | Buffer: 12909 | Max Q: 5.758\n",
      "Episode  423 | Reward:  44.00 | Steps: 12953 | Avg Loss: 6.0143 | Epsilon: 0.108 | Buffer: 12953 | Max Q: 2.659\n",
      "\n",
      "--- Episode 424: Updated target network. ---\n",
      "Episode  424 | Reward:  82.00 | Steps: 13035 | Avg Loss: 6.3189 | Epsilon: 0.107 | Buffer: 13035 | Max Q: 6.557\n",
      "Episode  425 | Reward:  76.00 | Steps: 13111 | Avg Loss: 7.2128 | Epsilon: 0.107 | Buffer: 13111 | Max Q: 15.418\n",
      "Episode  426 | Reward:  75.00 | Steps: 13186 | Avg Loss: 6.4719 | Epsilon: 0.106 | Buffer: 13186 | Max Q: 4.768\n",
      "Episode  427 | Reward:  51.00 | Steps: 13237 | Avg Loss: 6.4758 | Epsilon: 0.106 | Buffer: 13237 | Max Q: 3.808\n",
      "\n",
      "--- Episode 428: Updated target network. ---\n",
      "Episode  428 | Reward:  43.00 | Steps: 13280 | Avg Loss: 6.6775 | Epsilon: 0.105 | Buffer: 13280 | Max Q: 2.128\n",
      "Episode  429 | Reward:  94.00 | Steps: 13374 | Avg Loss: 7.2123 | Epsilon: 0.105 | Buffer: 13374 | Max Q: 16.649\n",
      "Episode  430 | Reward:  69.00 | Steps: 13443 | Avg Loss: 7.3179 | Epsilon: 0.104 | Buffer: 13443 | Max Q: 6.290\n",
      "\n",
      "--- Episode 431: Updated target network. ---\n",
      "Episode  431 | Reward:  93.00 | Steps: 13536 | Avg Loss: 7.1157 | Epsilon: 0.104 | Buffer: 13536 | Max Q: 4.234\n",
      "Episode  432 | Reward:  59.00 | Steps: 13595 | Avg Loss: 7.4330 | Epsilon: 0.103 | Buffer: 13595 | Max Q: 7.767\n",
      "Episode  433 | Reward:  62.00 | Steps: 13657 | Avg Loss: 7.3705 | Epsilon: 0.103 | Buffer: 13657 | Max Q: 13.098\n",
      "Episode  434 | Reward:  79.00 | Steps: 13736 | Avg Loss: 6.6772 | Epsilon: 0.102 | Buffer: 13736 | Max Q: 13.648\n",
      "\n",
      "--- Episode 435: Updated target network. ---\n",
      "Episode  435 | Reward:  65.00 | Steps: 13801 | Avg Loss: 7.1380 | Epsilon: 0.102 | Buffer: 13801 | Max Q: 15.131\n",
      "Episode  436 | Reward: 129.00 | Steps: 13930 | Avg Loss: 7.5034 | Epsilon: 0.101 | Buffer: 13930 | Max Q: 15.409\n",
      "Episode  437 | Reward:  45.00 | Steps: 13975 | Avg Loss: 7.3600 | Epsilon: 0.101 | Buffer: 13975 | Max Q: 1.312\n",
      "\n",
      "--- Episode 438: Updated target network. ---\n",
      "Episode  438 | Reward:  48.00 | Steps: 14023 | Avg Loss: 8.0619 | Epsilon: 0.100 | Buffer: 14023 | Max Q: 4.038\n",
      "Episode  439 | Reward:  61.00 | Steps: 14084 | Avg Loss: 7.6850 | Epsilon: 0.100 | Buffer: 14084 | Max Q: 3.863\n",
      "Episode  440 | Reward:  82.00 | Steps: 14166 | Avg Loss: 7.8071 | Epsilon: 0.099 | Buffer: 14166 | Max Q: 2.479\n",
      "\n",
      "--- Episode 441: Updated target network. ---\n",
      "Episode  441 | Reward:  88.00 | Steps: 14254 | Avg Loss: 7.6465 | Epsilon: 0.099 | Buffer: 14254 | Max Q: 6.255\n",
      "Episode  442 | Reward:  26.00 | Steps: 14280 | Avg Loss: 8.0952 | Epsilon: 0.098 | Buffer: 14280 | Max Q: 5.751\n",
      "Episode  443 | Reward:  93.00 | Steps: 14373 | Avg Loss: 7.9677 | Epsilon: 0.098 | Buffer: 14373 | Max Q: 9.054\n",
      "Episode  444 | Reward:  67.00 | Steps: 14440 | Avg Loss: 8.6111 | Epsilon: 0.097 | Buffer: 14440 | Max Q: 5.214\n",
      "Episode  445 | Reward:  58.00 | Steps: 14498 | Avg Loss: 7.6116 | Epsilon: 0.097 | Buffer: 14498 | Max Q: 14.517\n",
      "\n",
      "--- Episode 446: Updated target network. ---\n",
      "Episode  446 | Reward:  78.00 | Steps: 14576 | Avg Loss: 7.5586 | Epsilon: 0.096 | Buffer: 14576 | Max Q: 7.448\n",
      "Episode  447 | Reward:  94.00 | Steps: 14670 | Avg Loss: 8.1444 | Epsilon: 0.096 | Buffer: 14670 | Max Q: 2.191\n",
      "\n",
      "--- Episode 448: Updated target network. ---\n",
      "Episode  448 | Reward: 112.00 | Steps: 14782 | Avg Loss: 7.7209 | Epsilon: 0.095 | Buffer: 14782 | Max Q: 1.919\n",
      "Episode  449 | Reward:  74.00 | Steps: 14856 | Avg Loss: 7.8634 | Epsilon: 0.095 | Buffer: 14856 | Max Q: 10.087\n",
      "Episode  450 | Reward:  83.00 | Steps: 14939 | Avg Loss: 7.7281 | Epsilon: 0.094 | Buffer: 14939 | Max Q: 7.641\n",
      "\n",
      "--- Episode 451: Updated target network. ---\n",
      "Episode  451 | Reward:  88.00 | Steps: 15027 | Avg Loss: 7.9442 | Epsilon: 0.094 | Buffer: 15027 | Max Q: 7.831\n",
      "Episode  452 | Reward:  70.00 | Steps: 15097 | Avg Loss: 8.2943 | Epsilon: 0.093 | Buffer: 15097 | Max Q: 5.393\n",
      "Episode  453 | Reward:  69.00 | Steps: 15166 | Avg Loss: 7.8771 | Epsilon: 0.093 | Buffer: 15166 | Max Q: 10.662\n",
      "\n",
      "--- Episode 454: Updated target network. ---\n",
      "Episode  454 | Reward:  85.00 | Steps: 15251 | Avg Loss: 7.9408 | Epsilon: 0.092 | Buffer: 15251 | Max Q: 19.160\n",
      "Episode  455 | Reward:  69.00 | Steps: 15320 | Avg Loss: 7.1377 | Epsilon: 0.092 | Buffer: 15320 | Max Q: 2.869\n",
      "Episode  456 | Reward:  59.00 | Steps: 15379 | Avg Loss: 6.4503 | Epsilon: 0.092 | Buffer: 15379 | Max Q: 2.927\n",
      "Episode  457 | Reward: 102.00 | Steps: 15481 | Avg Loss: 6.8827 | Epsilon: 0.091 | Buffer: 15481 | Max Q: 7.337\n",
      "\n",
      "--- Episode 458: Updated target network. ---\n",
      "Episode  458 | Reward:  93.00 | Steps: 15574 | Avg Loss: 6.8385 | Epsilon: 0.091 | Buffer: 15574 | Max Q: 6.165\n",
      "Episode  459 | Reward:  99.00 | Steps: 15673 | Avg Loss: 6.5757 | Epsilon: 0.090 | Buffer: 15673 | Max Q: 6.025\n",
      "\n",
      "--- Episode 460: Updated target network. ---\n",
      "Episode  460 | Reward: 103.00 | Steps: 15776 | Avg Loss: 6.4377 | Epsilon: 0.090 | Buffer: 15776 | Max Q: 6.558\n",
      "Episode  461 | Reward:  62.00 | Steps: 15838 | Avg Loss: 7.3880 | Epsilon: 0.089 | Buffer: 15838 | Max Q: 6.053\n",
      "Episode  462 | Reward: 103.00 | Steps: 15941 | Avg Loss: 7.3510 | Epsilon: 0.089 | Buffer: 15941 | Max Q: 10.685\n",
      "Episode  463 | Reward:  58.00 | Steps: 15999 | Avg Loss: 7.6775 | Epsilon: 0.088 | Buffer: 15999 | Max Q: 8.319\n",
      "\n",
      "--- Episode 464: Updated target network. ---\n",
      "Episode  464 | Reward: 113.00 | Steps: 16112 | Avg Loss: 7.5022 | Epsilon: 0.088 | Buffer: 16112 | Max Q: 8.937\n",
      "Episode  465 | Reward:  77.00 | Steps: 16189 | Avg Loss: 6.7114 | Epsilon: 0.087 | Buffer: 16189 | Max Q: 8.740\n",
      "\n",
      "--- Episode 466: Updated target network. ---\n",
      "Episode  466 | Reward: 107.00 | Steps: 16296 | Avg Loss: 6.9941 | Epsilon: 0.087 | Buffer: 16296 | Max Q: 9.703\n",
      "Episode  467 | Reward:  68.00 | Steps: 16364 | Avg Loss: 7.5152 | Epsilon: 0.087 | Buffer: 16364 | Max Q: 11.480\n",
      "Episode  468 | Reward:  75.00 | Steps: 16439 | Avg Loss: 6.6944 | Epsilon: 0.086 | Buffer: 16439 | Max Q: 8.457\n",
      "\n",
      "--- Episode 469: Updated target network. ---\n",
      "Episode  469 | Reward:  80.00 | Steps: 16519 | Avg Loss: 7.0066 | Epsilon: 0.086 | Buffer: 16519 | Max Q: 7.629\n",
      "Episode  470 | Reward:  63.00 | Steps: 16582 | Avg Loss: 7.3306 | Epsilon: 0.085 | Buffer: 16582 | Max Q: 15.713\n",
      "Episode  471 | Reward: 118.00 | Steps: 16700 | Avg Loss: 7.4223 | Epsilon: 0.085 | Buffer: 16700 | Max Q: 3.192\n",
      "\n",
      "--- Episode 472: Updated target network. ---\n",
      "Episode  472 | Reward:  99.00 | Steps: 16799 | Avg Loss: 7.1263 | Epsilon: 0.084 | Buffer: 16799 | Max Q: 0.203\n",
      "Episode  473 | Reward:  72.00 | Steps: 16871 | Avg Loss: 7.1006 | Epsilon: 0.084 | Buffer: 16871 | Max Q: 1.853\n",
      "Episode  474 | Reward:  58.00 | Steps: 16929 | Avg Loss: 7.2358 | Epsilon: 0.084 | Buffer: 16929 | Max Q: 1.875\n",
      "\n",
      "--- Episode 475: Updated target network. ---\n",
      "Episode  475 | Reward:  84.00 | Steps: 17013 | Avg Loss: 7.1162 | Epsilon: 0.083 | Buffer: 17013 | Max Q: 8.054\n",
      "Episode  476 | Reward:  72.00 | Steps: 17085 | Avg Loss: 8.5601 | Epsilon: 0.083 | Buffer: 17085 | Max Q: 4.497\n",
      "Episode  477 | Reward:  89.00 | Steps: 17174 | Avg Loss: 8.9067 | Epsilon: 0.082 | Buffer: 17174 | Max Q: 0.798\n",
      "\n",
      "--- Episode 478: Updated target network. ---\n",
      "Episode  478 | Reward: 103.00 | Steps: 17277 | Avg Loss: 8.0955 | Epsilon: 0.082 | Buffer: 17277 | Max Q: 12.203\n",
      "Episode  479 | Reward:  88.00 | Steps: 17365 | Avg Loss: 8.5483 | Epsilon: 0.082 | Buffer: 17365 | Max Q: 9.315\n",
      "\n",
      "--- Episode 480: Updated target network. ---\n",
      "Episode  480 | Reward: 151.00 | Steps: 17516 | Avg Loss: 8.5021 | Epsilon: 0.081 | Buffer: 17516 | Max Q: 9.522\n",
      "Episode  481 | Reward: 110.00 | Steps: 17626 | Avg Loss: 9.2339 | Epsilon: 0.081 | Buffer: 17626 | Max Q: 6.445\n",
      "Episode  482 | Reward:  59.00 | Steps: 17685 | Avg Loss: 9.4167 | Epsilon: 0.080 | Buffer: 17685 | Max Q: 1.187\n",
      "\n",
      "--- Episode 483: Updated target network. ---\n",
      "Episode  483 | Reward:  75.00 | Steps: 17760 | Avg Loss: 9.2872 | Epsilon: 0.080 | Buffer: 17760 | Max Q: 0.192\n",
      "Episode  484 | Reward:  84.00 | Steps: 17844 | Avg Loss: 8.5846 | Epsilon: 0.080 | Buffer: 17844 | Max Q: 16.406\n",
      "Episode  485 | Reward:  71.00 | Steps: 17915 | Avg Loss: 8.2197 | Epsilon: 0.079 | Buffer: 17915 | Max Q: 1.091\n",
      "\n",
      "--- Episode 486: Updated target network. ---\n",
      "Episode  486 | Reward:  87.00 | Steps: 18002 | Avg Loss: 7.7619 | Epsilon: 0.079 | Buffer: 18002 | Max Q: 1.635\n",
      "Episode  487 | Reward: 100.00 | Steps: 18102 | Avg Loss: 7.9609 | Epsilon: 0.078 | Buffer: 18102 | Max Q: 7.147\n",
      "Episode  488 | Reward:  75.00 | Steps: 18177 | Avg Loss: 7.8412 | Epsilon: 0.078 | Buffer: 18177 | Max Q: 6.616\n",
      "Episode  489 | Reward:  72.00 | Steps: 18249 | Avg Loss: 7.1732 | Epsilon: 0.078 | Buffer: 18249 | Max Q: 6.612\n",
      "\n",
      "--- Episode 490: Updated target network. ---\n",
      "Episode  490 | Reward: 150.00 | Steps: 18399 | Avg Loss: 7.1113 | Epsilon: 0.077 | Buffer: 18399 | Max Q: 9.741\n",
      "Episode  491 | Reward:  86.00 | Steps: 18485 | Avg Loss: 7.0995 | Epsilon: 0.077 | Buffer: 18485 | Max Q: 7.250\n",
      "\n",
      "--- Episode 492: Updated target network. ---\n",
      "Episode  492 | Reward: 151.00 | Steps: 18636 | Avg Loss: 7.8008 | Epsilon: 0.076 | Buffer: 18636 | Max Q: 3.149\n",
      "Episode  493 | Reward: 100.00 | Steps: 18736 | Avg Loss: 7.5336 | Epsilon: 0.076 | Buffer: 18736 | Max Q: 2.428\n",
      "\n",
      "--- Episode 494: Updated target network. ---\n",
      "Episode  494 | Reward:  80.00 | Steps: 18816 | Avg Loss: 6.6805 | Epsilon: 0.076 | Buffer: 18816 | Max Q: 1.432\n",
      "Episode  495 | Reward:  79.00 | Steps: 18895 | Avg Loss: 6.6860 | Epsilon: 0.075 | Buffer: 18895 | Max Q: 1.401\n",
      "Episode  496 | Reward:  63.00 | Steps: 18958 | Avg Loss: 6.8199 | Epsilon: 0.075 | Buffer: 18958 | Max Q: 2.836\n",
      "\n",
      "--- Episode 497: Updated target network. ---\n",
      "Episode  497 | Reward:  86.00 | Steps: 19044 | Avg Loss: 6.7752 | Epsilon: 0.075 | Buffer: 19044 | Max Q: 1.645\n",
      "Episode  498 | Reward:  64.00 | Steps: 19108 | Avg Loss: 6.4707 | Epsilon: 0.074 | Buffer: 19108 | Max Q: 5.473\n",
      "Episode  499 | Reward: 130.00 | Steps: 19238 | Avg Loss: 6.7599 | Epsilon: 0.074 | Buffer: 19238 | Max Q: 5.786\n",
      "\n",
      "--- Episode 500: Updated target network. ---\n",
      "Saved checkpoint at episode 500\n",
      "Episode  500 | Reward:  81.00 | Steps: 19319 | Avg Loss: 6.3996 | Epsilon: 0.073 | Buffer: 19319 | Max Q: 2.183\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Training Loop ---\n",
    "\n",
    "# Environment setup\n",
    "# Use render_mode=\"rgb_array\" to get the image frame\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Hyperparameters and setup\n",
    "BUFFER_CAPACITY = 20000\n",
    "buffer = ReplayBuffer(capacity=BUFFER_CAPACITY)\n",
    "NET_ACTION_SPACE = env.action_space.n # 2 for CartPole: Left or Right\n",
    "net = ConvNet(action_space=NET_ACTION_SPACE)\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Target Network setup (for stability)\n",
    "target_net = copy.deepcopy(net)\n",
    "target_net.eval()\n",
    "num_episodes = 500\n",
    "epsilon = 0.9\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.05\n",
    "target_update_freq = 250 # steps\n",
    "\n",
    "# Optimization setup\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "frame_stack = deque(maxlen=4)\n",
    "step_count = 0\n",
    "\n",
    "print(\"Starting DQN Training (CNN on Frames)\")\n",
    "for episode in range(num_episodes):\n",
    "    # 1. Environment Reset and Initial State setup\n",
    "    obs, info = env.reset()\n",
    "    frame = env.render()\n",
    "    processed = resizer(frame) # 84x84 NumPy array\n",
    "\n",
    "    frame_stack.clear()\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(processed) # Populate the deque with the initial frame\n",
    "\n",
    "    state_tensor = stack_to_tensor(frame_stack) # (1, 4, 84, 84)\n",
    "    state_for_buffer = np.stack(list(frame_stack), axis=0) # (4, 84, 84) NumPy array\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_loss = []\n",
    "    q_values_at_end = None # To store the final Q-values for logging\n",
    "\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "\n",
    "        # 2. Target Network Update\n",
    "        if step_count % target_update_freq == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "            print(f\"\\n--- Episode {episode+1}: Updated target network. ---\")\n",
    "\n",
    "        # 3. Action Selection (Epsilon-Greedy)\n",
    "        with torch.no_grad():\n",
    "            q_values = net(state_tensor)\n",
    "        action = epsilon_greedy_action_selection(q_values, epsilon)\n",
    "\n",
    "        # 4. Environment Step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Reward shaping (as in your original code)\n",
    "        # if done:\n",
    "        #     reward = -10\n",
    "        total_reward += reward\n",
    "\n",
    "        # 5. Next State Processing\n",
    "        frame_next = env.render()\n",
    "        processed_next = resizer(frame_next) # 84x84 NumPy array\n",
    "        frame_stack.append(processed_next)\n",
    "\n",
    "        next_state_tensor = stack_to_tensor(frame_stack) # (1, 4, 84, 84)\n",
    "        next_state_for_buffer = np.stack(list(frame_stack), axis=0) # (4, 84, 84) NumPy array\n",
    "        \n",
    "        # 6. Store Transition\n",
    "        buffer.push(state_for_buffer, action, reward, next_state_for_buffer, done)\n",
    "\n",
    "        # 7. Train Network\n",
    "        loss = train_step(buffer, net, target_net, optimizer, batch_size, gamma)\n",
    "        if loss is not None:\n",
    "            episode_loss.append(loss)\n",
    "\n",
    "        # 8. State Update\n",
    "        state_tensor = next_state_tensor\n",
    "        state_for_buffer = next_state_for_buffer # Must update for the next push\n",
    "        q_values_at_end = q_values # Store for final logging\n",
    "\n",
    "    # End of Episode Logging and Decay\n",
    "\n",
    "    # Checkpoint (using a modulus that makes sense for the number of episodes)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        torch.save(net.state_dict(), f\"dqn_checkpoint_{episode+1}.pt\")\n",
    "        print(f\"Saved checkpoint at episode {episode+1}\")\n",
    "\n",
    "    # Epsilon decay\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    avg_loss = sum(episode_loss) / len(episode_loss) if episode_loss else 0\n",
    "    max_q_val = q_values_at_end.max().item() if q_values_at_end is not None else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode+1:4d} | Reward: {total_reward:6.2f} | \"\n",
    "        f\"Steps: {step_count} | Avg Loss: {avg_loss:.4f} | \"\n",
    "        f\"Epsilon: {epsilon:.3f} | Buffer: {len(buffer):5d} | \"\n",
    "        f\"Max Q: {max_q_val:.3f}\"\n",
    "    )\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba7e3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_net = net\n",
    "eval_net.eval()  # disables dropout / batchnorm if any\n",
    "import imageio\n",
    "\n",
    "frames = []\n",
    "\n",
    "obs, info = env.reset()\n",
    "frame = env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "# initialize frame stack with 4 copies of the first processed frame\n",
    "processed = resizer(frame)\n",
    "frame_stack = deque(maxlen=4)\n",
    "for _ in range(4):\n",
    "    frame_stack.append(processed)\n",
    "\n",
    "state_tensor = stack_to_tensor(frame_stack)\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        q_values = eval_net(state_tensor)\n",
    "    action = q_values.argmax(dim=1).item()  # greedy\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "    frame_next = env.render()\n",
    "    frames.append(frame_next)\n",
    "\n",
    "    processed_next = resizer(frame_next)\n",
    "    frame_stack.append(processed_next)\n",
    "    state_tensor = stack_to_tensor(frame_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02573e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation GIF. Total Reward: 103.0\n"
     ]
    }
   ],
   "source": [
    "imageio.mimsave('eval_cartpole.gif', frames, fps=30)\n",
    "print(\"Saved evaluation GIF. Total Reward:\", total_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midas-py310-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
