{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6023898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import ale_py  as ale\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2533ac49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], shape=(210, 160, 3), dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\",frameskip=5)\n",
    "\n",
    "\n",
    "frame, info= env.reset()\n",
    "# plt.imshow(frame[0])\n",
    "# plt.axis('off')\n",
    "frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28a8993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_resize(frame):\n",
    "    gray = 0.299 * frame[:, :, 0] \\\n",
    "         + 0.587 * frame[:, :, 1] \\\n",
    "         + 0.114 * frame[:, :, 2]\n",
    "    gray = gray.astype(np.uint8)\n",
    "\n",
    "    img = Image.fromarray(gray)\n",
    "    img = img.resize((84, 84), Image.BILINEAR)\n",
    "\n",
    "    return np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=4):\n",
    "    resized_frame = preprocess_and_resize(frame)\n",
    "        \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.float32) for _ in range(stack_size)], maxlen=stack_size)\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(resized_frame)\n",
    "    else:\n",
    "        stacked_frames.append(resized_frame)\n",
    "    \n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "254f024f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_frame = preprocess_and_resize(frame)\n",
    "plt.imshow(p_frame, cmap='gray')\n",
    "plt.axis('off')\n",
    "p_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f12c3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_state, stacked_frames = stack_frames(deque(), frame, True)\n",
    "stacked_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67cd435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 1.0  # no-op, but makes intent explicit\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ccf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "\n",
    "def epsilon_greedy_action_selection(state, policy_net, epsilon, num_actions):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state)\n",
    "        return q_values.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4da81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\",frameskip=5)\n",
    "\n",
    "frame, info= env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "525fbdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy \n",
    "num_actions = env.action_space.n\n",
    "q_net = DQN(input_channels=4, num_actions=num_actions)\n",
    "# Assuming q_net is your main Q-network\n",
    "target_net = copy.deepcopy(q_net)\n",
    "target_net.eval()  # Target net is only used for inference\n",
    "\n",
    "\n",
    "# Move networks to device first\n",
    "q_net.to(device)\n",
    "target_net.to(device)\n",
    "\n",
    "# state, stacked_frames = stack_frames(None, frame, True)\n",
    "# state_tensor = torch.from_numpy(state).unsqueeze(0)  # (1, 4, 84, 84)\n",
    "# state_tensor = state_tensor.float()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     q_values = q_net(state_tensor)\n",
    "\n",
    "# print(q_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebdfc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "episodes = 10000\n",
    "batch_size = 64\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.9955\n",
    "min_epsilon = 0.1\n",
    "gamma = 0.995\n",
    "lr = 0.1e3\n",
    "buffer_size=1000000\n",
    "target_update_freq = 2000  # update target net every 1000 steps\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "buffer = ReplayBuffer(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04315ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   2%|â–         | 169/10000 [07:42<7:27:23,  2.73s/ep, Reward=2, Steps=202, Epsilon=0.467]"
     ]
    }
   ],
   "source": [
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(f\"runs/pacman_dqn/{run_id}\")\n",
    "\n",
    "global_step = 0\n",
    "loss_ma = 0  # moving average of loss\n",
    "# assign tqdm object\n",
    "pbar = trange(episodes, desc=\"Episodes\", unit=\"ep\", leave=False)\n",
    "for episode in pbar:\n",
    "    frame, info = env.reset()\n",
    "    state, stacked_frames = stack_frames(None, frame, True)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    episode_loss = 0.0\n",
    "    episode_q_mean = 0.0\n",
    "    episode_q_max = 0.0\n",
    "    episode_grad_norm = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(state, q_net, epsilon, num_actions)\n",
    "\n",
    "        next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_frame, False)\n",
    "\n",
    "        # Replay buffer\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        buffer.add(experience=experience)\n",
    "\n",
    "        # --- TD loss step ---\n",
    "        if buffer.size() > batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "\n",
    "            states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(device)\n",
    "            actions = torch.LongTensor(np.array([exp[1] for exp in batch])).to(device)\n",
    "            rewards = torch.FloatTensor(np.array([exp[2] for exp in batch])).to(device)\n",
    "            next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(device)\n",
    "            dones = torch.FloatTensor([float(exp[4]) for exp in batch]).to(device)\n",
    "\n",
    "            # Current Q-values\n",
    "            q_values_all = q_net(states)\n",
    "            q_values = q_values_all.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_actions = q_net(next_states).argmax(dim=1)\n",
    "                \n",
    "                next_q_values = target_net(next_states).gather(\n",
    "                    1, next_actions.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                target = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Loss\n",
    "            loss = F.smooth_l1_loss(q_values, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Optional: gradient norm\n",
    "            grad_norm = 0\n",
    "            for param in q_net.parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm += param.grad.data.norm(2).item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Moving average of loss\n",
    "            loss_ma = 0.99 * loss_ma + 0.01 * loss.item()\n",
    "\n",
    "            # --- TensorBoard logging ---\n",
    "            # Accumulate episode stats\n",
    "            episode_loss += loss.item()\n",
    "            episode_q_mean += q_values.mean().item()\n",
    "            episode_q_max += q_values.max().item()\n",
    "            episode_grad_norm += grad_norm\n",
    "            train_steps += 1\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "        # -------------------\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    # Optional: decay epsilon here\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Dynamically update metrics inline\n",
    "    pbar.set_postfix({\n",
    "        \"Reward\": total_reward,\n",
    "        \"Steps\": step,\n",
    "        \"Epsilon\": f\"{epsilon:.3f}\"\n",
    "    })\n",
    "    if train_steps > 0:  # buffer was warm, training actually happened\n",
    "        writer.add_scalar(\"Episode/total_reward\", total_reward, episode)\n",
    "        writer.add_scalar(\"Episode/length\", step, episode)\n",
    "        writer.add_scalar(\"Episode/mean_td_loss\", episode_loss / train_steps, episode)\n",
    "        writer.add_scalar(\"Episode/mean_q\", episode_q_mean / train_steps, episode)\n",
    "        writer.add_scalar(\"Episode/max_q\", episode_q_max / train_steps, episode)\n",
    "        writer.add_scalar(\"Episode/mean_grad_norm\", episode_grad_norm / train_steps, episode)\n",
    "        writer.add_scalar(\"Episode/epsilon\", epsilon, episode)\n",
    "\n",
    "\n",
    "    if (episode + 1) % 250 == 0:\n",
    "        checkpoint = {\n",
    "            \"episode\": episode + 1,\n",
    "            \"q_net_state\": q_net.state_dict(),\n",
    "            \"target_net_state\": target_net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"epsilon\": epsilon,\n",
    "            \"global_step\": global_step\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoints/pacman_dqn_ep{episode+1}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, q_net, target_net, optimizer, device=\"cuda\"):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    q_net.load_state_dict(checkpoint[\"q_net\"])\n",
    "    target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    return (\n",
    "        checkpoint[\"episode\"],\n",
    "        checkpoint[\"epsilon\"],\n",
    "        checkpoint[\"step_count\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0f44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from episode 250\n",
      "Epsilon: 0.8825, Global step: 107916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91930\\AppData\\Local\\Temp\\ipykernel_3516\\3579235307.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"checkpoints/pacman_dqn_ep250.pth\"  # change as needed\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "q_net.load_state_dict(checkpoint[\"q_net_state\"])\n",
    "target_net.load_state_dict(checkpoint[\"target_net_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "start_episode = checkpoint[\"episode\"]\n",
    "epsilon = checkpoint[\"epsilon\"]\n",
    "global_step = checkpoint[\"global_step\"]\n",
    "\n",
    "print(f\"Resumed from episode {start_episode}\")\n",
    "print(f\"Epsilon: {epsilon:.4f}, Global step: {global_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c994e733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-521.1310, -495.9741, -482.5642, -496.4282, -464.8993, -588.1917,\n",
      "         -500.2617, -494.8692, -473.5265]], device='cuda:0') torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 4, 84, 84).to(device)  # batch=1\n",
    "    q = q_net(dummy)\n",
    "    print(q, q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27f4b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    env,\n",
    "    q_net,\n",
    "    num_episodes=10,\n",
    "    render=False,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    q_net.eval()          # important\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "\n",
    "    with torch.no_grad(): # absolutely critical\n",
    "        for ep in range(num_episodes):\n",
    "            frame, info = env.reset()\n",
    "            state, stacked_frames = stack_frames(None, frame, True)\n",
    "\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                state_t = (\n",
    "                    torch.from_numpy(state)\n",
    "                    .unsqueeze(0)\n",
    "                    .float()\n",
    "                    .to(device)\n",
    "                )\n",
    "\n",
    "                # action = q_net(state_t).argmax(dim=1).item()\n",
    "                action = epsilon_greedy_action_selection(\n",
    "                    state, q_net, epsilon=0.3, num_actions=env.action_space.n\n",
    "                )\n",
    "\n",
    "                next_frame, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                state, stacked_frames = stack_frames(\n",
    "                    stacked_frames, next_frame, False\n",
    "                )\n",
    "\n",
    "                ep_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "            rewards.append(ep_reward)\n",
    "            lengths.append(steps)\n",
    "            print(f\"[EVAL] Ep {ep+1:02d} | Reward {ep_reward:4.1f} | Steps {steps}\")\n",
    "\n",
    "    q_net.train()  # restore mode for later training\n",
    "\n",
    "    print(\n",
    "        f\"\\n[EVAL SUMMARY] \"\n",
    "        f\"mean={np.mean(rewards):.2f}, \"\n",
    "        f\"max={np.max(rewards):.1f}, \"\n",
    "        f\"std={np.std(rewards):.2f}\"\n",
    "    )\n",
    "\n",
    "    return rewards, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ee8cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Ep 01 | Reward 280.0 | Steps 313\n",
      "[EVAL] Ep 02 | Reward 270.0 | Steps 473\n",
      "[EVAL] Ep 03 | Reward 340.0 | Steps 660\n",
      "[EVAL] Ep 04 | Reward 220.0 | Steps 396\n",
      "[EVAL] Ep 05 | Reward 180.0 | Steps 386\n",
      "[EVAL] Ep 06 | Reward 370.0 | Steps 565\n",
      "[EVAL] Ep 07 | Reward 250.0 | Steps 449\n",
      "[EVAL] Ep 08 | Reward 380.0 | Steps 607\n",
      "[EVAL] Ep 09 | Reward 560.0 | Steps 721\n",
      "[EVAL] Ep 10 | Reward 380.0 | Steps 452\n",
      "\n",
      "[EVAL SUMMARY] mean=323.00, max=560.0, std=103.06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([280.0, 270.0, 340.0, 220.0, 180.0, 370.0, 250.0, 380.0, 560.0, 380.0],\n",
       " [313, 473, 660, 396, 386, 565, 449, 607, 721, 452])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = gym.make(\"MsPacman-v4\", render_mode=\"rgb_array\",frameskip=5)\n",
    "\n",
    "\n",
    "evaluate_agent(\n",
    "    eval_env,\n",
    "    q_net,\n",
    "    num_episodes=10,\n",
    "    render=False,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e316a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Ep 01 | Reward 230.0 | Steps 381\n",
      "[EVAL] Ep 02 | Reward 300.0 | Steps 388\n",
      "[EVAL] Ep 03 | Reward 900.0 | Steps 639\n",
      "[EVAL] Ep 04 | Reward 300.0 | Steps 652\n",
      "[EVAL] Ep 05 | Reward 280.0 | Steps 423\n",
      "[EVAL] Ep 06 | Reward 300.0 | Steps 436\n",
      "[EVAL] Ep 07 | Reward 280.0 | Steps 373\n",
      "[EVAL] Ep 08 | Reward 240.0 | Steps 517\n",
      "[EVAL] Ep 09 | Reward 810.0 | Steps 578\n",
      "[EVAL] Ep 10 | Reward 290.0 | Steps 429\n",
      "[EVAL] Ep 11 | Reward 240.0 | Steps 418\n",
      "[EVAL] Ep 12 | Reward 340.0 | Steps 540\n",
      "[EVAL] Ep 13 | Reward 1780.0 | Steps 658\n",
      "[EVAL] Ep 14 | Reward 240.0 | Steps 489\n",
      "[EVAL] Ep 15 | Reward 270.0 | Steps 421\n",
      "[EVAL] Ep 16 | Reward 320.0 | Steps 441\n",
      "[EVAL] Ep 17 | Reward 390.0 | Steps 617\n",
      "[EVAL] Ep 18 | Reward 220.0 | Steps 426\n",
      "[EVAL] Ep 19 | Reward 380.0 | Steps 433\n",
      "[EVAL] Ep 20 | Reward 540.0 | Steps 586\n",
      "[EVAL] Ep 21 | Reward 270.0 | Steps 410\n",
      "[EVAL] Ep 22 | Reward 330.0 | Steps 402\n",
      "[EVAL] Ep 23 | Reward 250.0 | Steps 367\n",
      "[EVAL] Ep 24 | Reward 530.0 | Steps 495\n",
      "[EVAL] Ep 25 | Reward 310.0 | Steps 434\n",
      "[EVAL] Ep 26 | Reward 270.0 | Steps 617\n",
      "[EVAL] Ep 27 | Reward 380.0 | Steps 429\n",
      "[EVAL] Ep 28 | Reward 350.0 | Steps 449\n",
      "[EVAL] Ep 29 | Reward 330.0 | Steps 497\n",
      "[EVAL] Ep 30 | Reward 270.0 | Steps 396\n",
      "[EVAL] Ep 31 | Reward 260.0 | Steps 489\n",
      "[EVAL] Ep 32 | Reward 300.0 | Steps 449\n",
      "[EVAL] Ep 33 | Reward 340.0 | Steps 415\n",
      "[EVAL] Ep 34 | Reward 630.0 | Steps 511\n",
      "[EVAL] Ep 35 | Reward 340.0 | Steps 410\n",
      "[EVAL] Ep 36 | Reward 250.0 | Steps 353\n",
      "[EVAL] Ep 37 | Reward 300.0 | Steps 468\n",
      "[EVAL] Ep 38 | Reward 250.0 | Steps 356\n",
      "[EVAL] Ep 39 | Reward 280.0 | Steps 423\n",
      "[EVAL] Ep 40 | Reward 180.0 | Steps 410\n",
      "[EVAL] Ep 41 | Reward 280.0 | Steps 389\n",
      "[EVAL] Ep 42 | Reward 250.0 | Steps 471\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecordVideo\n\u001b[32m      3\u001b[39m eval_env = RecordVideo(\n\u001b[32m      4\u001b[39m     env,\n\u001b[32m      5\u001b[39m     video_folder=\u001b[33m\"\u001b[39m\u001b[33mvideos/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     episode_trigger=\u001b[38;5;28;01mlambda\u001b[39;00m e: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m eval_env.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mevaluate_agent\u001b[39m\u001b[34m(env, q_net, num_episodes, render, device)\u001b[39m\n\u001b[32m     34\u001b[39m next_frame, reward, terminated, truncated, _ = env.step(action)\n\u001b[32m     35\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m state, stacked_frames = \u001b[43mstack_frames\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstacked_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m ep_reward += reward\n\u001b[32m     42\u001b[39m steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mstack_frames\u001b[39m\u001b[34m(stacked_frames, frame, is_new_episode, stack_size)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstack_frames\u001b[39m(stacked_frames, frame, is_new_episode, stack_size=\u001b[32m4\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     resized_frame = \u001b[43mpreprocess_and_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_new_episode:\n\u001b[32m     16\u001b[39m         stacked_frames = deque([np.zeros((\u001b[32m84\u001b[39m, \u001b[32m84\u001b[39m), dtype=np.float32) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(stack_size)], maxlen=stack_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpreprocess_and_resize\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_and_resize\u001b[39m(frame):\n\u001b[32m      2\u001b[39m     gray = \u001b[32m0.299\u001b[39m * frame[:, :, \u001b[32m0\u001b[39m] \\\n\u001b[32m      3\u001b[39m          + \u001b[32m0.587\u001b[39m * frame[:, :, \u001b[32m1\u001b[39m] \\\n\u001b[32m      4\u001b[39m          + \u001b[32m0.114\u001b[39m * frame[:, :, \u001b[32m2\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     gray = \u001b[43mgray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     img = Image.fromarray(gray)\n\u001b[32m      8\u001b[39m     img = img.resize((\u001b[32m84\u001b[39m, \u001b[32m84\u001b[39m), Image.BILINEAR)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"videos/\",\n",
    "    episode_trigger=lambda e: True\n",
    ")\n",
    "\n",
    "evaluate_agent(\n",
    "    eval_env,\n",
    "    q_net,\n",
    "    num_episodes=50,\n",
    "    render=False,\n",
    "    device=device\n",
    ")\n",
    "eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gymnasium[other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c345d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midas-py310-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
